{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Triton Inference Server\n",
    "\n",
    "[Triton Inference Server](https://developer.nvidia.com/triton-inference-server) is an open-source project by NVIDIA for high-performance ML model deployment. In this section, we will practice deploying models using Triton; after you have finished, you should be able to:\n",
    "\n",
    "-   serve a model using Triton Inference Server with Python backend\n",
    "-   use dynamic batching to improve performance\n",
    "-   scale your model to run on multiple GPUs, and/or with multiple instances on the same GPU\n",
    "-   benchmark the Triton service, and recognize indications of potential problems\n",
    "-   and use optimized backends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anatomy of a Triton model with Python backend\n",
    "\n",
    "To start, run\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "mkdir ~/serve-system-chi/models/\n",
    "cp -r ~/serve-system-chi/models_staging/food_classifier ~/serve-system-chi/models/\n",
    "```\n",
    "\n",
    "to copy [our first configuration](https://github.com/teaching-on-testbeds/serve-system-chi/tree/main/models_staging/food_classifier) into the directory from which Triton will load models.\n",
    "\n",
    "Our initial implementation serves our food image classifier using PyTorch. Here’s how it works.\n",
    "\n",
    "In the [Dockerfile](https://github.com/teaching-on-testbeds/serve-system-chi/blob/main/docker/Dockerfile.triton), the Triton server is started with the command\n",
    "\n",
    "``` bash\n",
    "tritonserver --model-repository=/models\n",
    "```\n",
    "\n",
    "where the `/models` directry is organized as follows:\n",
    "\n",
    "    models/\n",
    "    └── food_classifier\n",
    "        ├── 1\n",
    "        │   ├── food11.pth\n",
    "        │   └── model.py\n",
    "        └── config.pbtxt\n",
    "\n",
    "It includes:\n",
    "\n",
    "-   a top-level directory whose name is the “model name”\n",
    "-   a configuration file `config.pbtxt` inside that directory. We’ll look at that shortly.\n",
    "-   and a subdirectory for each model version. We have model version 1, so we have a subdirectory 1. Inside this directory is a `model.py`, which describes how the model will run.\n",
    "\n",
    "Let’s [look at the configuration file first](https://github.com/teaching-on-testbeds/serve-system-chi/blob/main/models_staging/food_classifier/config.pbtxt). Here are the contents of `config.pbtxt`:\n",
    "\n",
    "    name: \"food_classifier\"\n",
    "    backend: \"python\"\n",
    "    max_batch_size: 16\n",
    "    input [\n",
    "      {\n",
    "        name: \"INPUT_IMAGE\"\n",
    "        data_type: TYPE_STRING\n",
    "        dims: [1]\n",
    "      }\n",
    "    ]\n",
    "    output [\n",
    "      {\n",
    "        name: \"FOOD_LABEL\"\n",
    "        data_type: TYPE_STRING\n",
    "        dims: [1]\n",
    "      },\n",
    "      {\n",
    "        name: \"PROBABILITY\"\n",
    "        data_type: TYPE_FP32\n",
    "        dims: [1]\n",
    "      }\n",
    "    ]\n",
    "      instance_group [\n",
    "        {\n",
    "          count: 1\n",
    "          kind: KIND_GPU\n",
    "          gpus: [ 0 ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "We have defined:\n",
    "\n",
    "-   a `name`, which must match the directory name\n",
    "-   a `backend` - we are using the basic [Python backend](https://github.com/triton-inference-server/python_backend). This is a highly flexible backend which allows us to define how our model will run by providing Python code in a `model.py` file.\n",
    "-   a `max_batch_size` - we have set it to 16, but generally you would set this according to the GPU memory available\n",
    "-   the `name`, `data_type`, and `dims` (dimensions) of each `input` to the model\n",
    "-   the `name`, `data_type`, and `dims` (dimensions) of each `output` from the model\n",
    "-   an `instance_group` with the `count` (number of copies of the model that we want to serve) and details of the device we want to serve it on (we will serve it on GPU 0). Note that to run the model on CPU instead, we could have used\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "      instance_group [\n",
    "        {\n",
    "          count: 1\n",
    "          kind: KIND_CPU\n",
    "        }\n",
    "      ]\n",
    "\n",
    "Next, let’s [look at `model.py`](https://github.com/teaching-on-testbeds/serve-system-chi/blob/main/models_staging/food_classifier/1/model.py). For a Triton model with Python backend, the `model.py` must define a class named `TritonPythonModel` with at least an `initialize` and `execute` method. Ours has:\n",
    "\n",
    "-   An `initialize` method to load the model, move it to the device specified in the `args` passed from the Triton server, and put it in inference mode. This will run as soon as Triton starts and loads models from the directory passed to it:\n",
    "\n",
    "``` python\n",
    "def initialize(self, args):\n",
    "        model_dir = os.path.dirname(__file__)\n",
    "        model_path = os.path.join(model_dir, \"food11.pth\")\n",
    "        \n",
    "        # From args, get info about what device the model is supposed to be on\n",
    "        instance_kind = args.get(\"model_instance_kind\", \"cpu\").lower()\n",
    "        if instance_kind == \"gpu\":\n",
    "            device_id = int(args.get(\"model_instance_device_id\", 0))\n",
    "            torch.cuda.set_device(device_id)\n",
    "            self.device = torch.device(f\"cuda:{device_id}\" if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "\n",
    "        self.model = torch.load(model_path, map_location=self.device, weights_only=False)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        self.classes = np.array([\n",
    "            \"Bread\", \"Dairy product\", \"Dessert\", \"Egg\", \"Fried food\",\n",
    "            \"Meat\", \"Noodles/Pasta\", \"Rice\", \"Seafood\", \"Soup\",\n",
    "            \"Vegetable/Fruit\"\n",
    "        ])\n",
    "```\n",
    "\n",
    "-   A `preprocess` method, which will run on each input image that is passed:\n",
    "\n",
    "``` python\n",
    "def preprocess(self, image_data):\n",
    "    if isinstance(image_data, str):\n",
    "        image_data = base64.b64decode(image_data)\n",
    "\n",
    "    if isinstance(image_data, bytes):\n",
    "        image_data = image_data.decode(\"utf-8\")\n",
    "        image_data = base64.b64decode(image_data)\n",
    "\n",
    "    image = Image.open(io.BytesIO(image_data)).convert('RGB')\n",
    "\n",
    "    img_tensor = self.transform(image).unsqueeze(0)\n",
    "    return img_tensor\n",
    "```\n",
    "\n",
    "-   and an `execute`, which will apply to batches of requests sent to this model:\n",
    "\n",
    "``` python\n",
    "def execute(self, requests):\n",
    "    # Gather inputs from all requests\n",
    "    batched_inputs = []\n",
    "    for request in requests:\n",
    "        in_tensor = pb_utils.get_input_tensor_by_name(request, \"INPUT_IMAGE\")\n",
    "        input_data_array = in_tensor.as_numpy()  # each assumed to be shape [1]\n",
    "        # Preprocess each input (resulting in a tensor of shape [1, C, H, W])\n",
    "        batched_inputs.append(self.preprocess(input_data_array[0, 0]))\n",
    "    \n",
    "    # Combine inputs along the batch dimension\n",
    "    batched_tensor = torch.cat(batched_inputs, dim=0).to(self.device)\n",
    "    print(\"BatchSize: \", len(batched_inputs))\n",
    "    # Run inference once on the full batch\n",
    "    with torch.no_grad():\n",
    "        outputs = self.model(batched_tensor)\n",
    "    \n",
    "    # Process the outputs and split them for each request\n",
    "    responses = []\n",
    "    for i, request in enumerate(requests):\n",
    "        output = outputs[i:i+1]  # select the i-th output\n",
    "        prob, predicted_class = torch.max(output, 1)\n",
    "        predicted_label = self.classes[predicted_class.item()]\n",
    "        probability = torch.sigmoid(prob).item()\n",
    "        \n",
    "        # Create numpy arrays with shape [1, 1] for consistency.\n",
    "        out_label_np = np.array([[predicted_label]], dtype=object)\n",
    "        out_prob_np = np.array([[probability]], dtype=np.float32)\n",
    "        \n",
    "        out_tensor_label = pb_utils.Tensor(\"FOOD_LABEL\", out_label_np)\n",
    "        out_tensor_prob = pb_utils.Tensor(\"PROBABILITY\", out_prob_np)\n",
    "        \n",
    "        inference_response = pb_utils.InferenceResponse(\n",
    "            output_tensors=[out_tensor_label, out_tensor_prob])\n",
    "        responses.append(inference_response)\n",
    "    \n",
    "    return responses\n",
    "```\n",
    "\n",
    "Finally, now that we understand how the server works, let’s [look at how the Flask app sends requests to it](https://github.com/teaching-on-testbeds/gourmetgram/blob/triton/app.py). Inside the Flask app, we now have a function which is called whenever there is a new image uploaded to `predict` or `test`, which sends the image to the Triton server:\n",
    "\n",
    "``` python\n",
    "def request_triton(image_path):\n",
    "    try:\n",
    "        # Connect to Triton server\n",
    "        triton_client = httpclient.InferenceServerClient(url=TRITON_SERVER_URL)\n",
    "\n",
    "        # Prepare inputs and outputs\n",
    "        with open(image_path, 'rb') as f:\n",
    "            image_bytes = f.read()\n",
    "\n",
    "        inputs = []\n",
    "        inputs.append(httpclient.InferInput(\"INPUT_IMAGE\", [1, 1], \"BYTES\"))\n",
    "\n",
    "        encoded_str =  base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "        input_data = np.array([[encoded_str]], dtype=object)\n",
    "        inputs[0].set_data_from_numpy(input_data)\n",
    "\n",
    "        outputs = []\n",
    "        outputs.append(httpclient.InferRequestedOutput(\"FOOD_LABEL\", binary_data=False))\n",
    "        outputs.append(httpclient.InferRequestedOutput(\"PROBABILITY\", binary_data=False))\n",
    "\n",
    "        # Run inference\n",
    "        results = triton_client.infer(model_name=FOOD11_MODEL_NAME, inputs=inputs, outputs=outputs)\n",
    "\n",
    "        predicted_class = results.as_numpy(\"FOOD_LABEL\")[0,0]\n",
    "        probability = results.as_numpy(\"PROBABILITY\")[0,0]\n",
    "\n",
    "        return predicted_class, probability\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference: {e}\")  \n",
    "        return None, None  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring up containers\n",
    "\n",
    "To start, run\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "docker compose -f ~/serve-system-chi/docker/docker-compose-triton.yaml up -d\n",
    "```\n",
    "\n",
    "This uses a [Docker Compose configuration](https://github.com/teaching-on-testbeds/serve-system-chi/blob/main/docker/docker-compose-triton.yaml) to bring up three containers:\n",
    "\n",
    "-   one container with NVIDIA Triton Server, with the host’s GPUs passed to the container, and with the `models` directory (containing the model and its configuration) passed as a bind mount\n",
    "-   one container that hosts the Flask app, which will serve the user interface and send inference requests to the Triton server\n",
    "-   one Jupyter container with the Triton client installed, for us to conduct a performance evaluation of the Triton server\n",
    "\n",
    "Watch the logs from the Triton server as it starts up:\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "docker logs triton_server -f\n",
    "```\n",
    "\n",
    "Once the Triton server starts up, you should see something like\n",
    "\n",
    "    +--------------------------+---------+--------+\n",
    "    | Model                    | Version | Status |\n",
    "    +--------------------------+---------+--------+\n",
    "    | food_classifier | 1       | READY  |\n",
    "    +--------------------------+---------+--------+\n",
    "\n",
    "and then some additional output. Near the end, you will see\n",
    "\n",
    "    \"Started GRPCInferenceService at 0.0.0.0:8001\"\n",
    "    \"Started HTTPService at 0.0.0.0:8000\"\n",
    "    \"Started Metrics Service at 0.0.0.0:8002\"\n",
    "\n",
    "(and then some messages about not getting GPU power consumption, which is fine and not a concern.)\n",
    "\n",
    "You can use Ctrl+C to stop watching the logs once you see this output.\n",
    "\n",
    "Let’s test this service. In a browser, run\n",
    "\n",
    "    http://A.B.C.D\n",
    "\n",
    "but substitute the floating IP assigned to your instance, to access the Flask app. Upload an image and press “Submit” to get its class label.\n",
    "\n",
    "Finally, check the logs of the Jupyter container:\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "docker logs jupyter\n",
    "```\n",
    "\n",
    "and look for a line like\n",
    "\n",
    "    http://127.0.0.1:8888/lab?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "\n",
    "Paste this into a browser tab, but in place of 127.0.0.1, substitute the floating IP assigned to your instance, to open the Jupyter notebook interface that is running *on your compute instance*.\n",
    "\n",
    "Then, in the file browser on the left side, open the “work” directory and then click on the `triton.ipynb` notebook to continue.\n",
    "\n",
    "Meanwhile, on the host, run\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "nvtop\n",
    "```\n",
    "\n",
    "to monitor GPU usage - we will refer back to this a few times as we run through the rest of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving a PyTorch model\n",
    "\n",
    "The Triton client comes with a performance analyzer, which we can use to send requests to the server and get some statistics back. Let’s try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Successfully read data for 1 stream/streams with 1 step/steps.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average latency and throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "\n",
      "Request concurrency: 1\n",
      "  Client: \n",
      "    Request count: 1035\n",
      "    Throughput: 57.1653 infer/sec\n",
      "    Avg latency: 17393 usec (standard deviation 324 usec)\n",
      "    p50 latency: 17333 usec\n",
      "    p90 latency: 17563 usec\n",
      "    p95 latency: 17797 usec\n",
      "    p99 latency: 18695 usec\n",
      "    Avg HTTP time: 17386 usec (send/recv 210 usec + response wait 17176 usec)\n",
      "  Server: \n",
      "    Inference count: 1035\n",
      "    Execution count: 1035\n",
      "    Successful request count: 1035\n",
      "    Avg request latency: 16593 usec (overhead 3 usec + queue 67 usec + compute input 55 usec + compute infer 16383 usec + compute output 83 usec)\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 57.1653 infer/sec, latency 17393 usec\n"
     ]
    }
   ],
   "source": [
    "# runs inside Jupyter container\n",
    "perf_analyzer -u triton_server:8000  -m food_classifier  --input-data input.json -b 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a note of the line showing the total average request latency, and the breakdown including:\n",
    "\n",
    "-   `queue`, the queuing delay\n",
    "-   and `compute infer`, the inference delay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\n",
    "    Avg request latency: 18689 usec (overhead 2 usec + queue 22 usec + compute input 44 usec + compute infer 18570 usec + compute output 49 usec)\n",
    "\n",
    "Inferences/Second vs. Client Average Batch Latency\n",
    "Concurrency: 1, throughput: 51.549 infer/sec, latency 19311 usec\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s further exercise this service. In the command above, a single client sends continuous requests to the server - each time a response is returned, a new request is generated. Now, let’s configure **8** concurrent clients, each sending continuous requests - as soon as any client gets a response, it sends a new request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Successfully read data for 1 stream/streams with 1 step/steps.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average latency and throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "\n",
      "Request concurrency: 1\n",
      "  Client: \n",
      "    Request count: 1030\n",
      "    Throughput: 56.9957 infer/sec\n",
      "    Avg latency: 17463 usec (standard deviation 926 usec)\n",
      "    p50 latency: 17317 usec\n",
      "    p90 latency: 17736 usec\n",
      "    p95 latency: 17984 usec\n",
      "    p99 latency: 18808 usec\n",
      "    Avg HTTP time: 17456 usec (send/recv 207 usec + response wait 17249 usec)\n",
      "  Server: \n",
      "    Inference count: 1030\n",
      "    Execution count: 1030\n",
      "    Successful request count: 1030\n",
      "    Avg request latency: 16677 usec (overhead 2 usec + queue 65 usec + compute input 54 usec + compute infer 16475 usec + compute output 80 usec)\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 56.9957 infer/sec, latency 17463 usec\n"
     ]
    }
   ],
   "source": [
    "# runs inside Jupyter container\n",
    "perf_analyzer -u triton_server:8000  -m food_classifier  --input-data input.json -b 1 --request-distribution=poisson --concurrency-range 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \n",
    "\n",
    "    Avg request latency: 151375 usec (overhead 3 usec + queue 132341 usec + compute input 59 usec + compute infer 18922 usec + compute output 49 usec)\n",
    "\n",
    "Inferences/Second vs. Client Average Batch Latency\n",
    "Concurrency: 8, throughput: 52.3786 infer/sec, latency 151983 usec\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the inference time (`compute infer`) remains low, the overall system latency is high because of `queue` delay. Only one sample is processed at a time, and other samples have to wait in a queue for their turn. Here, since there are 8 concurrent clients sending continuous requests, the delay is approximately 8x the inference delay. With more concurrent requests, the queuing delay would grow even larger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Successfully read data for 1 stream/streams with 1 step/steps.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average latency and throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Latency limit: 0 msec\n",
      "  Concurrency limit: 8 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "\n",
      "Request concurrency: 1\n",
      "  Client: \n",
      "    Request count: 1034\n",
      "    Throughput: 57.1949 infer/sec\n",
      "    Avg latency: 17390 usec (standard deviation 1336 usec)\n",
      "    p50 latency: 17185 usec\n",
      "    p90 latency: 17526 usec\n",
      "    p95 latency: 18181 usec\n",
      "    p99 latency: 21263 usec\n",
      "    Avg HTTP time: 17383 usec (send/recv 197 usec + response wait 17186 usec)\n",
      "  Server: \n",
      "    Inference count: 1034\n",
      "    Execution count: 1034\n",
      "    Successful request count: 1034\n",
      "    Avg request latency: 16608 usec (overhead 3 usec + queue 67 usec + compute input 56 usec + compute infer 16401 usec + compute output 79 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 9.84211%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5564 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 36.0877 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 727711744 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request concurrency: 2\n",
      "  Client: \n",
      "    Request count: 989\n",
      "    Throughput: 54.829 infer/sec\n",
      "    Avg latency: 36365 usec (standard deviation 1949 usec)\n",
      "    p50 latency: 36315 usec\n",
      "    p90 latency: 37437 usec\n",
      "    p95 latency: 38530 usec\n",
      "    p99 latency: 43883 usec\n",
      "    Avg HTTP time: 36358 usec (send/recv 205 usec + response wait 36153 usec)\n",
      "  Server: \n",
      "    Inference count: 989\n",
      "    Execution count: 989\n",
      "    Successful request count: 989\n",
      "    Avg request latency: 35574 usec (overhead 3 usec + queue 17396 usec + compute input 53 usec + compute infer 18039 usec + compute output 81 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 10.5263%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5566 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 36.5096 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 727711744 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request concurrency: 3\n",
      "  Client: \n",
      "    Request count: 1057\n",
      "    Throughput: 58.5814 infer/sec\n",
      "    Avg latency: 51077 usec (standard deviation 2716 usec)\n",
      "    p50 latency: 49697 usec\n",
      "    p90 latency: 55160 usec\n",
      "    p95 latency: 55751 usec\n",
      "    p99 latency: 57239 usec\n",
      "    Avg HTTP time: 51071 usec (send/recv 216 usec + response wait 50855 usec)\n",
      "  Server: \n",
      "    Inference count: 1058\n",
      "    Execution count: 1058\n",
      "    Successful request count: 1058\n",
      "    Avg request latency: 50273 usec (overhead 4 usec + queue 33260 usec + compute input 54 usec + compute infer 16874 usec + compute output 80 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 11.1579%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5579 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 36.7268 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 727711744 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request concurrency: 4\n",
      "  Client: \n",
      "    Request count: 1075\n",
      "    Throughput: 59.5783 infer/sec\n",
      "    Avg latency: 67036 usec (standard deviation 4279 usec)\n",
      "    p50 latency: 65829 usec\n",
      "    p90 latency: 69300 usec\n",
      "    p95 latency: 81166 usec\n",
      "    p99 latency: 83493 usec\n",
      "    Avg HTTP time: 67040 usec (send/recv 214 usec + response wait 66826 usec)\n",
      "  Server: \n",
      "    Inference count: 1074\n",
      "    Execution count: 1074\n",
      "    Successful request count: 1074\n",
      "    Avg request latency: 66256 usec (overhead 3 usec + queue 49521 usec + compute input 54 usec + compute infer 16597 usec + compute output 80 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 10.9474%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5569 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 36.4338 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 727711744 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request concurrency: 5\n",
      "  Client: \n",
      "    Request count: 1100\n",
      "    Throughput: 60.9626 infer/sec\n",
      "    Avg latency: 81856 usec (standard deviation 1651 usec)\n",
      "    p50 latency: 81923 usec\n",
      "    p90 latency: 82906 usec\n",
      "    p95 latency: 84677 usec\n",
      "    p99 latency: 86305 usec\n",
      "    Avg HTTP time: 81849 usec (send/recv 227 usec + response wait 81622 usec)\n",
      "  Server: \n",
      "    Inference count: 1101\n",
      "    Execution count: 1101\n",
      "    Successful request count: 1101\n",
      "    Avg request latency: 81037 usec (overhead 3 usec + queue 64687 usec + compute input 52 usec + compute infer 16218 usec + compute output 76 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 11.5263%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5587 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 36.6123 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 727711744 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request concurrency: 6\n",
      "  Client: \n",
      "    Request count: 1094\n",
      "    Throughput: 60.4912 infer/sec\n",
      "    Avg latency: 99014 usec (standard deviation 1924 usec)\n",
      "    p50 latency: 99237 usec\n",
      "    p90 latency: 100434 usec\n",
      "    p95 latency: 101916 usec\n",
      "    p99 latency: 103852 usec\n",
      "    Avg HTTP time: 99006 usec (send/recv 234 usec + response wait 98772 usec)\n",
      "  Server: \n",
      "    Inference count: 1094\n",
      "    Execution count: 1094\n",
      "    Successful request count: 1094\n",
      "    Avg request latency: 98186 usec (overhead 2 usec + queue 81712 usec + compute input 54 usec + compute infer 16338 usec + compute output 79 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 11.4737%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5579 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 36.4981 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 727711744 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request concurrency: 7\n",
      "  Client: \n",
      "    Request count: 1124\n",
      "    Throughput: 62.2766 infer/sec\n",
      "    Avg latency: 112240 usec (standard deviation 1509 usec)\n",
      "    p50 latency: 111993 usec\n",
      "    p90 latency: 113440 usec\n",
      "    p95 latency: 114684 usec\n",
      "    p99 latency: 117564 usec\n",
      "    Avg HTTP time: 112233 usec (send/recv 229 usec + response wait 112004 usec)\n",
      "  Server: \n",
      "    Inference count: 1124\n",
      "    Execution count: 1124\n",
      "    Successful request count: 1124\n",
      "    Avg request latency: 111408 usec (overhead 4 usec + queue 95401 usec + compute input 55 usec + compute infer 15871 usec + compute output 77 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 11.8947%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5582 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 36.5749 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 727711744 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request concurrency: 8\n",
      "  Client: \n",
      "    Request count: 1112\n",
      "    Throughput: 61.6243 infer/sec\n",
      "    Avg latency: 129689 usec (standard deviation 2407 usec)\n",
      "    p50 latency: 129457 usec\n",
      "    p90 latency: 132782 usec\n",
      "    p95 latency: 133545 usec\n",
      "    p99 latency: 135514 usec\n",
      "    Avg HTTP time: 129696 usec (send/recv 232 usec + response wait 129464 usec)\n",
      "  Server: \n",
      "    Inference count: 1111\n",
      "    Execution count: 1111\n",
      "    Successful request count: 1111\n",
      "    Avg request latency: 128874 usec (overhead 3 usec + queue 112689 usec + compute input 55 usec + compute infer 16049 usec + compute output 77 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 11.8947%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5579 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 36.5995 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 727711744 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 57.1949 infer/sec, latency 17390 usec\n",
      "Concurrency: 2, throughput: 54.829 infer/sec, latency 36365 usec\n",
      "Concurrency: 3, throughput: 58.5814 infer/sec, latency 51077 usec\n",
      "Concurrency: 4, throughput: 59.5783 infer/sec, latency 67036 usec\n",
      "Concurrency: 5, throughput: 60.9626 infer/sec, latency 81856 usec\n",
      "Concurrency: 6, throughput: 60.4912 infer/sec, latency 99014 usec\n",
      "Concurrency: 7, throughput: 62.2766 infer/sec, latency 112240 usec\n",
      "Concurrency: 8, throughput: 61.6243 infer/sec, latency 129689 usec\n"
     ]
    }
   ],
   "source": [
    "perf_analyzer -u triton_server:8000  -m food_classifier  --input-data input.json -b 1 --request-distribution=poisson --concurrency-range 1:8 --collect-metrics -f metrics_output.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Successfully read data for 1 stream/streams with 1 step/steps.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average latency and throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Latency limit: 0 msec\n",
      "  Concurrency limit: 8 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "\n",
      "Request concurrency: 1\n",
      "  Client: \n",
      "    Request count: 931\n",
      "    Throughput: 51.4832 infer/sec\n",
      "    Avg latency: 19321 usec (standard deviation 1501 usec)\n",
      "    p50 latency: 19045 usec\n",
      "    p90 latency: 19528 usec\n",
      "    p95 latency: 19992 usec\n",
      "    p99 latency: 24033 usec\n",
      "    Avg HTTP time: 19315 usec (send/recv 226 usec + response wait 19089 usec)\n",
      "  Server: \n",
      "    Inference count: 931\n",
      "    Execution count: 931\n",
      "    Successful request count: 931\n",
      "    Avg request latency: 18521 usec (overhead 4 usec + queue 66 usec + compute input 55 usec + compute infer 18315 usec + compute output 81 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 9.47368%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5574 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 35.7179 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 727711744 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request concurrency: 2\n",
      "  Client: \n",
      "    Request count: 985\n",
      "    Throughput: 54.5709 infer/sec\n",
      "    Avg latency: 36550 usec (standard deviation 815 usec)\n",
      "    p50 latency: 36497 usec\n",
      "    p90 latency: 37086 usec\n",
      "    p95 latency: 37465 usec\n",
      "    p99 latency: 38286 usec\n",
      "    Avg HTTP time: 36556 usec (send/recv 234 usec + response wait 36322 usec)\n",
      "  Server: \n",
      "    Inference count: 984\n",
      "    Execution count: 984\n",
      "    Successful request count: 984\n",
      "    Avg request latency: 35737 usec (overhead 2 usec + queue 17463 usec + compute input 54 usec + compute infer 18136 usec + compute output 81 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 10.4211%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.559 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 36.2804 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 727711744 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request concurrency: 3\n",
      "  Client: \n",
      "    Request count: 977\n",
      "    Throughput: 54.1632 infer/sec\n",
      "    Avg latency: 55250 usec (standard deviation 1023 usec)\n",
      "    p50 latency: 55028 usec\n",
      "    p90 latency: 56292 usec\n",
      "    p95 latency: 56949 usec\n",
      "    p99 latency: 57823 usec\n",
      "    Avg HTTP time: 55243 usec (send/recv 235 usec + response wait 55008 usec)\n",
      "  Server: \n",
      "    Inference count: 977\n",
      "    Execution count: 977\n",
      "    Successful request count: 977\n",
      "    Avg request latency: 54429 usec (overhead 3 usec + queue 36026 usec + compute input 56 usec + compute infer 18262 usec + compute output 81 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 10.1053%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5585 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 36.4082 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 727711744 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request concurrency: 4\n",
      "  Client: \n",
      "    Request count: 983\n",
      "    Throughput: 54.4972 infer/sec\n",
      "    Avg latency: 73248 usec (standard deviation 1153 usec)\n",
      "    p50 latency: 73005 usec\n",
      "    p90 latency: 74501 usec\n",
      "    p95 latency: 75308 usec\n",
      "    p99 latency: 76779 usec\n",
      "    Avg HTTP time: 73240 usec (send/recv 237 usec + response wait 73003 usec)\n",
      "  Server: \n",
      "    Inference count: 983\n",
      "    Execution count: 983\n",
      "    Successful request count: 983\n",
      "    Avg request latency: 72426 usec (overhead 3 usec + queue 54134 usec + compute input 54 usec + compute infer 18152 usec + compute output 81 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 10.2105%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5569 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 36.1909 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 727711744 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request concurrency: 5\n",
      "  Client: \n",
      "    Request count: 998\n",
      "    Throughput: 54.9438 infer/sec\n",
      "    Avg latency: 90819 usec (standard deviation 1427 usec)\n",
      "    p50 latency: 90912 usec\n",
      "    p90 latency: 92066 usec\n",
      "    p95 latency: 92965 usec\n",
      "    p99 latency: 95074 usec\n",
      "    Avg HTTP time: 90811 usec (send/recv 240 usec + response wait 90571 usec)\n",
      "  Server: \n",
      "    Inference count: 998\n",
      "    Execution count: 998\n",
      "    Successful request count: 998\n",
      "    Avg request latency: 89985 usec (overhead 3 usec + queue 71845 usec + compute input 55 usec + compute infer 18003 usec + compute output 78 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 10.6316%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5585 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 36.203 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 727711744 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request concurrency: 6\n",
      "  Client: \n",
      "    Request count: 989\n",
      "    Throughput: 54.8241 infer/sec\n",
      "    Avg latency: 109277 usec (standard deviation 1881 usec)\n",
      "    p50 latency: 108928 usec\n",
      "    p90 latency: 111413 usec\n",
      "    p95 latency: 112481 usec\n",
      "    p99 latency: 113833 usec\n",
      "    Avg HTTP time: 109270 usec (send/recv 230 usec + response wait 109040 usec)\n",
      "  Server: \n",
      "    Inference count: 989\n",
      "    Execution count: 989\n",
      "    Successful request count: 989\n",
      "    Avg request latency: 108482 usec (overhead 3 usec + queue 90294 usec + compute input 56 usec + compute infer 18047 usec + compute output 81 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 10.4737%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5577 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 36.0763 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 727711744 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request concurrency: 7\n",
      "  Client: \n",
      "    Request count: 990\n",
      "    Throughput: 54.8713 infer/sec\n",
      "    Avg latency: 127404 usec (standard deviation 1770 usec)\n",
      "    p50 latency: 127184 usec\n",
      "    p90 latency: 129529 usec\n",
      "    p95 latency: 130749 usec\n",
      "    p99 latency: 132405 usec\n",
      "    Avg HTTP time: 127397 usec (send/recv 237 usec + response wait 127160 usec)\n",
      "  Server: \n",
      "    Inference count: 990\n",
      "    Execution count: 990\n",
      "    Successful request count: 990\n",
      "    Avg request latency: 126582 usec (overhead 3 usec + queue 108410 usec + compute input 55 usec + compute infer 18033 usec + compute output 81 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 10.3158%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5574 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 36.369 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 727711744 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request concurrency: 8\n",
      "  Client: \n",
      "    Request count: 989\n",
      "    Throughput: 54.8147 infer/sec\n",
      "    Avg latency: 145651 usec (standard deviation 1867 usec)\n",
      "    p50 latency: 145425 usec\n",
      "    p90 latency: 147685 usec\n",
      "    p95 latency: 149331 usec\n",
      "    p99 latency: 151411 usec\n",
      "    Avg HTTP time: 145644 usec (send/recv 236 usec + response wait 145408 usec)\n",
      "  Server: \n",
      "    Inference count: 989\n",
      "    Execution count: 989\n",
      "    Successful request count: 989\n",
      "    Avg request latency: 144834 usec (overhead 4 usec + queue 126655 usec + compute input 54 usec + compute infer 18043 usec + compute output 77 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 10.6316%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5582 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 36.4974 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 727711744 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 51.4832 infer/sec, latency 19321 usec\n",
      "Concurrency: 2, throughput: 54.5709 infer/sec, latency 36550 usec\n",
      "Concurrency: 3, throughput: 54.1632 infer/sec, latency 55250 usec\n",
      "Concurrency: 4, throughput: 54.4972 infer/sec, latency 73248 usec\n",
      "Concurrency: 5, throughput: 54.9438 infer/sec, latency 90819 usec\n",
      "Concurrency: 6, throughput: 54.8241 infer/sec, latency 109277 usec\n",
      "Concurrency: 7, throughput: 54.8713 infer/sec, latency 127404 usec\n",
      "Concurrency: 8, throughput: 54.8147 infer/sec, latency 145651 usec\n"
     ]
    }
   ],
   "source": [
    "# runs inside Jupyter container\n",
    "perf_analyzer -u triton_server:8000  -m food_classifier  --input-data input.json -b 1 --request-distribution=poisson --concurrency-range 1:8 --log-frequency=10 --collect-metrics -f metrics_output_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Successfully read data for 1 stream/streams with 1 step/steps.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average latency and throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Latency limit: 0 msec\n",
      "  Concurrency limit: 8 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "\n",
      "Request concurrency: 1\n",
      "  Client: \n",
      "    Request count: 932\n",
      "    Throughput: 51.5819 infer/sec\n",
      "    Avg latency: 19274 usec (standard deviation 1565 usec)\n",
      "    p50 latency: 18980 usec\n",
      "    p90 latency: 19505 usec\n",
      "    p95 latency: 20193 usec\n",
      "    p99 latency: 23958 usec\n",
      "    Avg HTTP time: 19267 usec (send/recv 220 usec + response wait 19047 usec)\n",
      "  Server: \n",
      "    Inference count: 932\n",
      "    Execution count: 932\n",
      "    Successful request count: 932\n",
      "    Avg request latency: 18459 usec (overhead 3 usec + queue 66 usec + compute input 56 usec + compute infer 18254 usec + compute output 78 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 8.84211%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5564 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 35.6919 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 727711744 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request concurrency: 2\n",
      "  Client: \n",
      "    Request count: 1112\n",
      "    Throughput: 61.449 infer/sec\n",
      "    Avg latency: 32450 usec (standard deviation 366 usec)\n",
      "    p50 latency: 32357 usec\n",
      "    p90 latency: 32681 usec\n",
      "    p95 latency: 33004 usec\n",
      "    p99 latency: 34234 usec\n",
      "    Avg HTTP time: 32443 usec (send/recv 206 usec + response wait 32237 usec)\n",
      "  Server: \n",
      "    Inference count: 1112\n",
      "    Execution count: 1112\n",
      "    Successful request count: 1112\n",
      "    Avg request latency: 31634 usec (overhead 4 usec + queue 15424 usec + compute input 56 usec + compute infer 16073 usec + compute output 76 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 11.5789%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5566 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 36.9192 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 727711744 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request concurrency: 3\n",
      "  Client: \n",
      "    Request count: 1108\n",
      "    Throughput: 61.4178 infer/sec\n",
      "    Avg latency: 48710 usec (standard deviation 835 usec)\n",
      "    p50 latency: 48601 usec\n",
      "    p90 latency: 49047 usec\n",
      "    p95 latency: 49707 usec\n",
      "    p99 latency: 51081 usec\n",
      "    Avg HTTP time: 48702 usec (send/recv 211 usec + response wait 48491 usec)\n",
      "  Server: \n",
      "    Inference count: 1108\n",
      "    Execution count: 1108\n",
      "    Successful request count: 1108\n",
      "    Avg request latency: 47896 usec (overhead 3 usec + queue 31675 usec + compute input 55 usec + compute infer 16087 usec + compute output 74 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 12%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5835 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 36.7922 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 727711744 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request concurrency: 4\n",
      "  Client: \n",
      "    Request count: 1105\n",
      "    Throughput: 60.9718 infer/sec\n",
      "    Avg latency: 65473 usec (standard deviation 3052 usec)\n",
      "    p50 latency: 64497 usec\n",
      "    p90 latency: 68224 usec\n",
      "    p95 latency: 73187 usec\n",
      "    p99 latency: 74251 usec\n",
      "    Avg HTTP time: 65479 usec (send/recv 225 usec + response wait 65254 usec)\n",
      "  Server: \n",
      "    Inference count: 1104\n",
      "    Execution count: 1104\n",
      "    Successful request count: 1104\n",
      "    Avg request latency: 64666 usec (overhead 4 usec + queue 48316 usec + compute input 55 usec + compute infer 16213 usec + compute output 78 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 11.4737%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5585 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 36.6144 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 727711744 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request concurrency: 5\n",
      "  Client: \n",
      "    Request count: 1110\n",
      "    Throughput: 61.5266 infer/sec\n",
      "    Avg latency: 81165 usec (standard deviation 1257 usec)\n",
      "    p50 latency: 80829 usec\n",
      "    p90 latency: 82647 usec\n",
      "    p95 latency: 83943 usec\n",
      "    p99 latency: 85400 usec\n",
      "    Avg HTTP time: 81158 usec (send/recv 221 usec + response wait 80937 usec)\n",
      "  Server: \n",
      "    Inference count: 1110\n",
      "    Execution count: 1110\n",
      "    Successful request count: 1110\n",
      "    Avg request latency: 80358 usec (overhead 3 usec + queue 64149 usec + compute input 53 usec + compute infer 16071 usec + compute output 81 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 11.7895%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5569 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 36.7153 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 727711744 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request concurrency: 6\n",
      "  Client: \n",
      "    Request count: 1108\n",
      "    Throughput: 61.4088 infer/sec\n",
      "    Avg latency: 97576 usec (standard deviation 1230 usec)\n",
      "    p50 latency: 97185 usec\n",
      "    p90 latency: 99509 usec\n",
      "    p95 latency: 100451 usec\n",
      "    p99 latency: 101402 usec\n",
      "    Avg HTTP time: 97569 usec (send/recv 222 usec + response wait 97347 usec)\n",
      "  Server: \n",
      "    Inference count: 1108\n",
      "    Execution count: 1108\n",
      "    Successful request count: 1108\n",
      "    Avg request latency: 96774 usec (overhead 3 usec + queue 80539 usec + compute input 54 usec + compute infer 16096 usec + compute output 81 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 11.8947%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5574 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 36.8309 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 727711744 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request concurrency: 7\n",
      "  Client: \n",
      "    Request count: 1075\n",
      "    Throughput: 59.58 infer/sec\n",
      "    Avg latency: 117302 usec (standard deviation 7048 usec)\n",
      "    p50 latency: 115233 usec\n",
      "    p90 latency: 122067 usec\n",
      "    p95 latency: 135569 usec\n",
      "    p99 latency: 144737 usec\n",
      "    Avg HTTP time: 117295 usec (send/recv 224 usec + response wait 117071 usec)\n",
      "  Server: \n",
      "    Inference count: 1075\n",
      "    Execution count: 1075\n",
      "    Successful request count: 1075\n",
      "    Avg request latency: 116487 usec (overhead 3 usec + queue 99753 usec + compute input 49 usec + compute infer 16614 usec + compute output 67 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 11.2632%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.6091 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 36.5747 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 727711744 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request concurrency: 8\n",
      "  Client: \n",
      "    Request count: 1113\n",
      "    Throughput: 61.2343 infer/sec\n",
      "    Avg latency: 130397 usec (standard deviation 1797 usec)\n",
      "    p50 latency: 130072 usec\n",
      "    p90 latency: 132084 usec\n",
      "    p95 latency: 133205 usec\n",
      "    p99 latency: 135947 usec\n",
      "    Avg HTTP time: 130390 usec (send/recv 227 usec + response wait 130163 usec)\n",
      "  Server: \n",
      "    Inference count: 1113\n",
      "    Execution count: 1113\n",
      "    Successful request count: 1113\n",
      "    Avg request latency: 129572 usec (overhead 3 usec + queue 113295 usec + compute input 52 usec + compute infer 16151 usec + compute output 70 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 11.6842%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5841 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 36.9582 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 727711744 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 51.5819 infer/sec, latency 19274 usec\n",
      "Concurrency: 2, throughput: 61.449 infer/sec, latency 32450 usec\n",
      "Concurrency: 3, throughput: 61.4178 infer/sec, latency 48710 usec\n",
      "Concurrency: 4, throughput: 60.9718 infer/sec, latency 65473 usec\n",
      "Concurrency: 5, throughput: 61.5266 infer/sec, latency 81165 usec\n",
      "Concurrency: 6, throughput: 61.4088 infer/sec, latency 97576 usec\n",
      "Concurrency: 7, throughput: 59.58 infer/sec, latency 117302 usec\n",
      "Concurrency: 8, throughput: 61.2343 infer/sec, latency 130397 usec\n"
     ]
    }
   ],
   "source": [
    "perf_analyzer -u triton_server:8000  -m food_classifier  --input-data input.json -b 1 --request-distribution=poisson --concurrency-range 1:8 --collect-metrics -f metrics_output_test2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_analyzer -u triton_server:8000  -m food_classifier  --input-data input.json -b 1 --request-distribution=poisson --concurrency-range 1:8 --log-frequency=10 --collect-metrics -f metrics_output_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs inside Jupyter container\n",
    "perf_analyzer -u triton_server:8000  -m food_classifier  --input-data input.json -b 16 --concurrency-range 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Although the delay is large (over 100 ms), it’s not because of inadequate compute - if you check the `nvtop` display on the host while the test above is running, you will note low GPU utilization! Take a screenshot of the `nvtop` output when this test is running.\n",
    "\n",
    "We *could* get more throughput without increasing prediction latency, by batching requests:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Skipping the following part for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, that’s not very helpful in a situation when requests come from individual users, one at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic batching\n",
    "\n",
    "Earlier, we noted that our model can achieve higher throughput with low latency by performing inference on batches of input samples, instead of individual samples. However, our client sends requests with individual samples.\n",
    "\n",
    "To improve performance, we can ask the Triton server to batch incoming requests whenever possible, and send them through the server together instead of a sequence. In other words, if the server is ready to handle the next request, and it finds four requests waiting in the queue, it should serve those four as a batch instead of just taking the next request in line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"model_stats\":[{\"name\":\"food_classifier\",\"version\":\"1\",\"last_inference\":0,\"inference_count\":0,\"execution_count\":0,\"inference_stats\":{\"success\":{\"count\":0,\"ns\":0},\"fail\":{\"count\":0,\"ns\":0},\"queue\":{\"count\":0,\"ns\":0},\"compute_input\":{\"count\":0,\"ns\":0},\"compute_infer\":{\"count\":0,\"ns\":0},\"compute_output\":{\"count\":0,\"ns\":0},\"cache_hit\":{\"count\":0,\"ns\":0},\"cache_miss\":{\"count\":0,\"ns\":0}},\"response_stats\":{},\"batch_stats\":[],\"memory_usage\":[]}]}\n"
     ]
    }
   ],
   "source": [
    "# runs inside Jupyter container\n",
    "curl http://triton_server:8000/v2/models/food_classifier/versions/1/stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, run the benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Successfully read data for 1 stream/streams with 1 step/steps.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average latency and throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "\n",
      "Request concurrency: 8\n",
      "  Client: \n",
      "    Request count: 1081\n",
      "    Throughput: 59.7671 infer/sec\n",
      "    Avg latency: 133233 usec (standard deviation 26632 usec)\n",
      "    p50 latency: 129976 usec\n",
      "    p90 latency: 134445 usec\n",
      "    p95 latency: 134874 usec\n",
      "    p99 latency: 152350 usec\n",
      "    Avg HTTP time: 133221 usec (send/recv 204 usec + response wait 133017 usec)\n",
      "  Server: \n",
      "    Inference count: 1082\n",
      "    Execution count: 1082\n",
      "    Successful request count: 1082\n",
      "    Avg request latency: 132408 usec (overhead 2 usec + queue 115751 usec + compute input 58 usec + compute infer 16517 usec + compute output 79 usec)\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 8, throughput: 59.7671 infer/sec, latency 133233 usec\n"
     ]
    }
   ],
   "source": [
    "# runs inside Jupyter container\n",
    "perf_analyzer -u triton_server:8000  -m food_classifier  --input-data input.json -b 1 --concurrency-range 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\n",
    "    Avg request latency: 100423 usec (overhead 6 usec + queue 44892 usec + compute input 197 usec + compute infer 55111 usec + compute output 216 usec)\n",
    "\n",
    "Inferences/Second vs. Client Average Batch Latency\n",
    "Concurrency: 8, throughput: 78.6276 infer/sec, latency 101232 usec\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and get per-batch stats again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs inside Jupyter container\n",
    "curl http://triton_server:8000/v2/models/food_classifier/versions/1/stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\n",
    "{\"model_stats\":[{\"name\":\"food_classifier\",\"version\":\"1\",\"last_inference\":1741928954242,\"inference_count\":1436,\"execution_count\":386,\"inference_stats\":{\"success\":{\"count\":1436,\"ns\":144129653806},\"fail\":{\"count\":0,\"ns\":0},\"queue\":{\"count\":1436,\"ns\":64542800676},\"compute_input\":{\"count\":1436,\"ns\":283368073},\"compute_infer\":{\"count\":1436,\"ns\":78984688177},\"compute_output\":{\"count\":1436,\"ns\":309635270},\"cache_hit\":{\"count\":0,\"ns\":0},\"cache_miss\":{\"count\":0,\"ns\":0}},\"response_stats\":{},\"batch_stats\":[{\"batch_size\":1,\"compute_input\":{\"count\":26,\"ns\":1754466},\"compute_infer\":{\"count\":26,\"ns\":757012965},\"compute_output\":{\"count\":26,\"ns\":2038319}},{\"batch_size\":2,\"compute_input\":{\"count\":127,\"ns\":14474588},\"compute_infer\":{\"count\":127,\"ns\":3718519926},\"compute_output\":{\"count\":127,\"ns\":13184875}},{\"batch_size\":3,\"compute_input\":{\"count\":55,\"ns\":7182962},\"compute_infer\":{\"count\":55,\"ns\":2144383142},\"compute_output\":{\"count\":55,\"ns\":7683505}},{\"batch_size\":4,\"compute_input\":{\"count\":9,\"ns\":1446080},\"compute_infer\":{\"count\":9,\"ns\":456549788},\"compute_output\":{\"count\":9,\"ns\":1596636}},{\"batch_size\":5,\"compute_input\":{\"count\":73,\"ns\":14796021},\"compute_infer\":{\"count\":73,\"ns\":4268808423},\"compute_output\":{\"count\":73,\"ns\":16766209}},{\"batch_size\":6,\"compute_input\":{\"count\":82,\"ns\":19691717},\"compute_infer\":{\"count\":82,\"ns\":5577222019},\"compute_output\":{\"count\":82,\"ns\":22604780}},{\"batch_size\":7,\"compute_input\":{\"count\":14,\"ns\":4742974},\"compute_infer\":{\"count\":14,\"ns\":1103416079},\"compute_output\":{\"count\":14,\"ns\":4618631}}],\"memory_usage\":[]}]}\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the stats show that some requests were served in batch sizes greater than 1, even though each client sent a single request at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling up\n",
    "\n",
    "Another easy way to improve performance is to scale up! Let’s edit the model configuration:\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "nano ~/serve-system-chi/models/food_classifier/config.pbtxt\n",
    "```\n",
    "\n",
    "and change\n",
    "\n",
    "      instance_group [\n",
    "        {\n",
    "          count: 1\n",
    "          kind: KIND_GPU\n",
    "          gpus: [ 0 ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "to run two instances on GPU 0 and two instances on GPU 1:\n",
    "\n",
    "      instance_group [\n",
    "        {\n",
    "          count: 2\n",
    "          kind: KIND_GPU\n",
    "          gpus: [ 0 ]\n",
    "        },\n",
    "        {\n",
    "          count: 2\n",
    "          kind: KIND_GPU\n",
    "          gpus: [ 1 ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "Save the file (use Ctrl+O then Enter, then Ctrl+X).\n",
    "\n",
    "Re-build the container image with this change:\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "docker compose -f ~/serve-system-chi/docker/docker-compose-triton.yaml build triton_server\n",
    "```\n",
    "\n",
    "and then bring the server back up:\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "docker compose -f ~/serve-system-chi/docker/docker-compose-triton.yaml up triton_server --force-recreate -d\n",
    "```\n",
    "\n",
    "and use\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "docker logs triton_server\n",
    "```\n",
    "\n",
    "to make sure the server comes up and is ready.\n",
    "\n",
    "On the host, run\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "nvidia-smi\n",
    "```\n",
    "\n",
    "and note that there are two instances of `triton_python_backend` processes running on GPU 0, and two on GPU 1.\n",
    "\n",
    "Then, benchmark *this* service with increased concurrency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs inside Jupyter container\n",
    "perf_analyzer -u triton_server:8000  -m food_classifier  --input-data input.json -b 1 --concurrency-range 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \n",
    "\n",
    "    Avg request latency: 40707 usec (overhead 3 usec + queue 7036 usec + compute input 75 usec + compute infer 33514 usec + compute output 78 usec)\n",
    "\n",
    "Inferences/Second vs. Client Average Batch Latency\n",
    "Concurrency: 8, throughput: 192.849 infer/sec, latency 41374 usec\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there is still some queuing delay (because our degree of concurrency, 8, is still higher than the number of server instances, 4), and the inference time is also increased due to sharing the compute resources, the prediction delay is still on the order of 10s of ms - not over 100ms, like it was previously with concurrency 8!\n",
    "\n",
    "Also, if you look at the `nvtop` output on the host while running this test, you will observe higher GPU utilization than before (which is good! We want to use the GPU. Underutilization is bad.) (Take a screenshot!) However, we are still not fully utilizing the GPU.\n",
    "\n",
    "Let’s try increasing the number of instances again. Edit the model configuration:\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "nano ~/serve-system-chi/models/food_classifier/config.pbtxt\n",
    "```\n",
    "\n",
    "and change\n",
    "\n",
    "      instance_group [\n",
    "        {\n",
    "          count: 2\n",
    "          kind: KIND_GPU\n",
    "          gpus: [ 0 ]\n",
    "        },\n",
    "        {\n",
    "          count: 2\n",
    "          kind: KIND_GPU\n",
    "          gpus: [ 1 ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "to\n",
    "\n",
    "      instance_group [\n",
    "        {\n",
    "          count: 4\n",
    "          kind: KIND_GPU\n",
    "          gpus: [ 0 ]\n",
    "        },\n",
    "        {\n",
    "          count: 4\n",
    "          kind: KIND_GPU\n",
    "          gpus: [ 1 ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "Re-build the container image with this change:\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "docker compose -f ~/serve-system-chi/docker/docker-compose-triton.yaml build triton_server\n",
    "```\n",
    "\n",
    "and then bring the server back up:\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "docker compose -f ~/serve-system-chi/docker/docker-compose-triton.yaml up triton_server --force-recreate -d\n",
    "```\n",
    "\n",
    "use\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "docker logs triton_server\n",
    "```\n",
    "\n",
    "to make sure the server comes up and is ready.\n",
    "\n",
    "Then, re-run our benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs inside Jupyter container\n",
    "perf_analyzer -u triton_server:8000  -m food_classifier  --input-data input.json -b 1 --concurrency-range 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\n",
    "    Avg request latency: 66737 usec (overhead 2 usec + queue 466 usec + compute input 61 usec + compute infer 66118 usec + compute output 89 usec)\n",
    "\n",
    "Inferences/Second vs. Client Average Batch Latency\n",
    "Concurrency: 8, throughput: 118.688 infer/sec, latency 67559 usec\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes things worse - our inference time is higher, even though we are still underutilizing the GPU (as seen in `nvtop`) (take a screenshot!).\n",
    "\n",
    "Our system is not limited by GPU - we are underutilizing the GPU. However, we are being killed by the overhead of the Python backend and our `model.py` implementation.\n",
    "\n",
    "#Skipped up to here!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving an ONNX model\n",
    "\n",
    "The Python backend we have been using is flexible, but not necessarily the most performant. To get better performance, we will use one of the highly optimized backend in Triton. Since we already have an ONNX model, let’s use the ONNX backend.\n",
    "\n",
    "To serve a model using the ONNX backend, we will create a [directory structure like this](https://github.com/teaching-on-testbeds/serve-system-chi/tree/main/models_staging/food_classifier_onnx):\n",
    "\n",
    "    food_classifier_onnx/\n",
    "    ├── 1\n",
    "    │   └── model.onnx\n",
    "    └── config.pbtxt\n",
    "\n",
    "There is no more `model.py` - Triton serves the model directly, we just have to name it `model.onnx`. In [`config.pbtxt`](https://github.com/teaching-on-testbeds/serve-system-chi/blob/main/models_staging/food_classifier_onnx/config.pbtxt), we will specify the backend as `onnxruntime`:\n",
    "\n",
    "    name: \"food_classifier_onnx\"\n",
    "    backend: \"onnxruntime\"\n",
    "    max_batch_size: 16\n",
    "    input [\n",
    "      {\n",
    "        name: \"input\"  # has to match ONNX model's input name\n",
    "        data_type: TYPE_FP32\n",
    "        dims: [3, 224, 224]  # has to match ONNX input shape\n",
    "      }\n",
    "    ]\n",
    "    output [\n",
    "      {\n",
    "        name: \"output\"  # has to match ONNX model output name\n",
    "        data_type: TYPE_FP32  # output is a list of probabilities\n",
    "        dims: [11]  # \n",
    "      }\n",
    "    ]\n",
    "      instance_group [\n",
    "        {\n",
    "          count: 1\n",
    "          kind: KIND_GPU\n",
    "          gpus: [ 0 ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "Copy this to Triton’s models directory:\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "cp -r ~/summer2025/models/food_classifier_onnx ~/exp-chi/models/\n",
    "```\n",
    "\n",
    "Re-build the container image with this change:\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "docker compose -f ~/exp-chi/docker/docker/docker-compose.yaml build triton_server\n",
    "```\n",
    "\n",
    "and then bring the server back up:\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "docker compose -f ~/exp-chi/docker/docker/docker-compose.yaml up triton_server --force-recreate -d\n",
    "```\n",
    "\n",
    "use\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "docker logs triton_server\n",
    "```\n",
    "\n",
    "to make sure the server comes up and is ready. Note that the server will load two models: the original `food_classifier` with Python backend, and the `food_classifier_onnx` model we just added.\n",
    "\n",
    "Let’s benchmark our service. Our ONNX model won’t accept image bytes directly - it expects images that already have been pre-processed into arrays. So, our benchmark command will be a little bit different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average latency and throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "\n",
      "Request concurrency: 1\n",
      "  Client: \n",
      "    Request count: 2400\n",
      "    Throughput: 125.502 infer/sec\n",
      "    Avg latency: 7373 usec (standard deviation 868 usec)\n",
      "    p50 latency: 7388 usec\n",
      "    p90 latency: 8330 usec\n",
      "    p95 latency: 8442 usec\n",
      "    p99 latency: 9236 usec\n",
      "    Avg HTTP time: 7365 usec (send/recv 520 usec + response wait 6845 usec)\n",
      "  Server: \n",
      "    Inference count: 2400\n",
      "    Execution count: 2400\n",
      "    Successful request count: 2400\n",
      "    Avg request latency: 5285 usec (overhead 35 usec + queue 80 usec + compute input 231 usec + compute infer 4918 usec + compute output 19 usec)\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 125.502 infer/sec, latency 7373 usec\n"
     ]
    }
   ],
   "source": [
    "# runs inside Jupyter container\n",
    "perf_analyzer -u triton_server:8000  -m food_classifier_onnx -b 1 --shape IMAGE:3,224,224 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average latency and throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "\n",
      "Request concurrency: 1\n",
      "  Client: \n",
      "    Request count: 2400\n",
      "    Throughput: 125.502 infer/sec\n",
      "    Avg latency: 7373 usec (standard deviation 868 usec)\n",
      "    p50 latency: 7388 usec\n",
      "    p90 latency: 8330 usec\n",
      "    p95 latency: 8442 usec\n",
      "    p99 latency: 9236 usec\n",
      "    Avg HTTP time: 7365 usec (send/recv 520 usec + response wait 6845 usec)\n",
      "  Server: \n",
      "    Inference count: 2400\n",
      "    Execution count: 2400\n",
      "    Successful request count: 2400\n",
      "    Avg request latency: 5285 usec (overhead 35 usec + queue 80 usec + compute input 231 usec + compute infer 4918 usec + compute output 19 usec)\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 125.502 infer/sec, latency 7373 usec\n"
     ]
    }
   ],
   "source": [
    "# runs inside Jupyter container\n",
    "perf_analyzer -u triton_server:8000  -m food_classifier_onnx -b 1 --shape IMAGE:3,224,224 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average latency and throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Latency limit: 0 msec\n",
      "  Concurrency limit: 8 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "\n",
      "Request concurrency: 1\n",
      "  Client: \n",
      "    Request count: 2227\n",
      "    Throughput: 114.747 infer/sec\n",
      "    Avg latency: 7948 usec (standard deviation 907 usec)\n",
      "    p50 latency: 7758 usec\n",
      "    p90 latency: 9213 usec\n",
      "    p95 latency: 9415 usec\n",
      "    p99 latency: 9502 usec\n",
      "    Avg HTTP time: 7939 usec (send/recv 586 usec + response wait 7353 usec)\n",
      "  Server: \n",
      "    Inference count: 2227\n",
      "    Execution count: 2227\n",
      "    Successful request count: 2227\n",
      "    Avg request latency: 5581 usec (overhead 37 usec + queue 82 usec + compute input 238 usec + compute infer 5202 usec + compute output 21 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 23.7%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.557 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 40.854 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 687865856 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request concurrency: 2\n",
      "  Client: \n",
      "    Request count: 7153\n",
      "    Throughput: 340.663 infer/sec\n",
      "    Avg latency: 5320 usec (standard deviation 430 usec)\n",
      "    p50 latency: 5177 usec\n",
      "    p90 latency: 5648 usec\n",
      "    p95 latency: 6140 usec\n",
      "    p99 latency: 7101 usec\n",
      "    Avg HTTP time: 5312 usec (send/recv 414 usec + response wait 4898 usec)\n",
      "  Server: \n",
      "    Inference count: 7154\n",
      "    Execution count: 7154\n",
      "    Successful request count: 7154\n",
      "    Avg request latency: 3426 usec (overhead 18 usec + queue 621 usec + compute input 133 usec + compute infer 2644 usec + compute output 10 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 61.7143%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5828 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 65.5687 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 687865856 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request concurrency: 3\n",
      "  Client: \n",
      "    Request count: 7120\n",
      "    Throughput: 362.245 infer/sec\n",
      "    Avg latency: 7806 usec (standard deviation 481 usec)\n",
      "    p50 latency: 7846 usec\n",
      "    p90 latency: 8026 usec\n",
      "    p95 latency: 8228 usec\n",
      "    p99 latency: 8571 usec\n",
      "    Avg HTTP time: 7798 usec (send/recv 416 usec + response wait 7382 usec)\n",
      "  Server: \n",
      "    Inference count: 7121\n",
      "    Execution count: 7121\n",
      "    Successful request count: 7121\n",
      "    Avg request latency: 6048 usec (overhead 18 usec + queue 3303 usec + compute input 128 usec + compute infer 2588 usec + compute output 9 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 65.5714%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5349 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 69.2538 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 687865856 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request concurrency: 4\n",
      "  Client: \n",
      "    Request count: 7000\n",
      "    Throughput: 361.836 infer/sec\n",
      "    Avg latency: 10596 usec (standard deviation 560 usec)\n",
      "    p50 latency: 10623 usec\n",
      "    p90 latency: 10919 usec\n",
      "    p95 latency: 11157 usec\n",
      "    p99 latency: 11549 usec\n",
      "    Avg HTTP time: 10588 usec (send/recv 377 usec + response wait 10211 usec)\n",
      "  Server: \n",
      "    Inference count: 7001\n",
      "    Execution count: 7001\n",
      "    Successful request count: 7001\n",
      "    Avg request latency: 8749 usec (overhead 18 usec + queue 5995 usec + compute input 128 usec + compute infer 2598 usec + compute output 9 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 64.9524%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5345 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 70.0599 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 687865856 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request concurrency: 5\n",
      "  Client: \n",
      "    Request count: 7023\n",
      "    Throughput: 363.561 infer/sec\n",
      "    Avg latency: 13280 usec (standard deviation 549 usec)\n",
      "    p50 latency: 13303 usec\n",
      "    p90 latency: 13554 usec\n",
      "    p95 latency: 13893 usec\n",
      "    p99 latency: 14500 usec\n",
      "    Avg HTTP time: 13272 usec (send/recv 329 usec + response wait 12943 usec)\n",
      "  Server: \n",
      "    Inference count: 7023\n",
      "    Execution count: 7023\n",
      "    Successful request count: 7023\n",
      "    Avg request latency: 11253 usec (overhead 18 usec + queue 8512 usec + compute input 129 usec + compute infer 2583 usec + compute output 9 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 65.4%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5569 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 71.2277 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 687865856 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request concurrency: 6\n",
      "  Client: \n",
      "    Request count: 7110\n",
      "    Throughput: 364.563 infer/sec\n",
      "    Avg latency: 15961 usec (standard deviation 569 usec)\n",
      "    p50 latency: 15993 usec\n",
      "    p90 latency: 16252 usec\n",
      "    p95 latency: 16548 usec\n",
      "    p99 latency: 17188 usec\n",
      "    Avg HTTP time: 15952 usec (send/recv 361 usec + response wait 15591 usec)\n",
      "  Server: \n",
      "    Inference count: 7112\n",
      "    Execution count: 7112\n",
      "    Successful request count: 7112\n",
      "    Avg request latency: 14063 usec (overhead 18 usec + queue 11330 usec + compute input 131 usec + compute infer 2574 usec + compute output 9 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 65.5%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5833 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 71.1862 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 687865856 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request concurrency: 7\n",
      "  Client: \n",
      "    Request count: 7202\n",
      "    Throughput: 349.685 infer/sec\n",
      "    Avg latency: 19452 usec (standard deviation 716 usec)\n",
      "    p50 latency: 19622 usec\n",
      "    p90 latency: 19893 usec\n",
      "    p95 latency: 20090 usec\n",
      "    p99 latency: 20426 usec\n",
      "    Avg HTTP time: 19444 usec (send/recv 402 usec + response wait 19042 usec)\n",
      "  Server: \n",
      "    Inference count: 7203\n",
      "    Execution count: 7203\n",
      "    Successful request count: 7203\n",
      "    Avg request latency: 17294 usec (overhead 18 usec + queue 14444 usec + compute input 111 usec + compute infer 2710 usec + compute output 10 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 62.9091%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5346 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 70.3556 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 687865856 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request concurrency: 8\n",
      "  Client: \n",
      "    Request count: 7856\n",
      "    Throughput: 360.77 infer/sec\n",
      "    Avg latency: 21542 usec (standard deviation 839 usec)\n",
      "    p50 latency: 21531 usec\n",
      "    p90 latency: 22590 usec\n",
      "    p95 latency: 22776 usec\n",
      "    p99 latency: 23221 usec\n",
      "    Avg HTTP time: 21533 usec (send/recv 376 usec + response wait 21157 usec)\n",
      "  Server: \n",
      "    Inference count: 7856\n",
      "    Execution count: 7856\n",
      "    Successful request count: 7856\n",
      "    Avg request latency: 19774 usec (overhead 18 usec + queue 17012 usec + compute input 131 usec + compute infer 2603 usec + compute output 9 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 64.6667%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.555 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 71.305 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 687865856 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 114.747 infer/sec, latency 7948 usec\n",
      "Concurrency: 2, throughput: 340.663 infer/sec, latency 5320 usec\n",
      "Concurrency: 3, throughput: 362.245 infer/sec, latency 7806 usec\n",
      "Concurrency: 4, throughput: 361.836 infer/sec, latency 10596 usec\n",
      "Concurrency: 5, throughput: 363.561 infer/sec, latency 13280 usec\n",
      "Concurrency: 6, throughput: 364.563 infer/sec, latency 15961 usec\n",
      "Concurrency: 7, throughput: 349.685 infer/sec, latency 19452 usec\n",
      "Concurrency: 8, throughput: 360.77 infer/sec, latency 21542 usec\n"
     ]
    }
   ],
   "source": [
    "perf_analyzer -u triton_server:8000  -m food_classifier_onnx -b 1 --shape IMAGE:3,224,224 --request-distribution=poisson --concurrency-range=1:8 --collect-metrics -f metrics_onnx1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation of M/D/1 queue varying λ from 36.5-328.5 considering an average μ=365 (to get values for ρ in [0.1,0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average latency and throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Latency limit: 0 msec\n",
      "  Request Rate limit: 99 requests per seconds\n",
      "  Using poisson distribution on request generation\n",
      "  Using synchronous calls for inference\n",
      "\n",
      "Request Rate: 11 inference requests per second\n",
      "  Client: \n",
      "    Request count: 161\n",
      "    Throughput: 8.89096 infer/sec\n",
      "    Avg latency: 10257 usec (standard deviation 1565 usec)\n",
      "    p50 latency: 9837 usec\n",
      "    p90 latency: 10652 usec\n",
      "    p95 latency: 14947 usec\n",
      "    p99 latency: 16994 usec\n",
      "    Avg HTTP time: 10247 usec (send/recv 1062 usec + response wait 9185 usec)\n",
      "  Server: \n",
      "    Inference count: 161\n",
      "    Execution count: 161\n",
      "    Successful request count: 161\n",
      "    Avg request latency: 7253 usec (overhead 44 usec + queue 485 usec + compute input 201 usec + compute infer 6496 usec + compute output 26 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 1.94444%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5587 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 33.857 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 687865856 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request Rate: 22 inference requests per second\n",
      "  Client: \n",
      "    Request count: 389\n",
      "    Throughput: 21.1966 infer/sec\n",
      "    Avg latency: 10190 usec (standard deviation 1928 usec)\n",
      "    p50 latency: 9632 usec\n",
      "    p90 latency: 12183 usec\n",
      "    p95 latency: 15026 usec\n",
      "    p99 latency: 17727 usec\n",
      "    Avg HTTP time: 10181 usec (send/recv 1044 usec + response wait 9137 usec)\n",
      "  Server: \n",
      "    Inference count: 389\n",
      "    Execution count: 389\n",
      "    Successful request count: 389\n",
      "    Avg request latency: 7266 usec (overhead 43 usec + queue 726 usec + compute input 191 usec + compute infer 6281 usec + compute output 25 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 5.33333%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5828 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 34.5726 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 687865856 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request Rate: 33 inference requests per second\n",
      "Failed to obtain stable measurement.\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Request Rate: 11, throughput: 8.89096 infer/sec, latency 10257 usec\n",
      "Request Rate: 22, throughput: 21.1966 infer/sec, latency 10190 usec\n"
     ]
    }
   ],
   "source": [
    "  perf_analyzer -u triton_server:8000 \\\n",
    "  -m food_classifier_onnx \\\n",
    "  -b 1 \\\n",
    "  --shape IMAGE:3,224,224 \\\n",
    "  --request-rate-range 11:99:11 \\\n",
    "  --request-distribution=poisson \\\n",
    "  --collect-metrics \\\n",
    "  -f test_output.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run above gave an average μ of 20000 => adjusting λ for the test below to actually get ρ values between [0.1,0.9]\n",
    "#Simulation of M/D/1 queue varying λ from 2000-18000 considering an average μ=365 (to get values for ρ in [0.1,0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Running for ρ=0.1 (λ=10 req/sec) at fixed concurrency \n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average latency and throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Latency limit: 0 msec\n",
      "  Request Rate limit: 10 requests per seconds\n",
      "  Using poisson distribution on request generation\n",
      "  Using synchronous calls for inference\n",
      "\n",
      "Request Rate: 10 inference requests per second\n",
      "  Client: \n",
      "    Request count: 182\n",
      "    Throughput: 10.0259 infer/sec\n",
      "    Avg latency: 10016 usec (standard deviation 1013 usec)\n",
      "    p50 latency: 9949 usec\n",
      "    p90 latency: 10168 usec\n",
      "    p95 latency: 10347 usec\n",
      "    p99 latency: 15253 usec\n",
      "    Avg HTTP time: 10007 usec (send/recv 1043 usec + response wait 8964 usec)\n",
      "  Server: \n",
      "    Inference count: 182\n",
      "    Execution count: 182\n",
      "    Successful request count: 182\n",
      "    Avg request latency: 7011 usec (overhead 45 usec + queue 234 usec + compute input 218 usec + compute infer 6487 usec + compute output 25 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 1.33333%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5584 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 33.8962 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 687865856 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Request Rate: 10, throughput: 10.0259 infer/sec, latency 10016 usec\n",
      "✅ Logged for ρ=0.1\n",
      "▶ Running for ρ=0.2 (λ=20 req/sec) at fixed concurrency \n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average latency and throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Latency limit: 0 msec\n",
      "  Request Rate limit: 20 requests per seconds\n",
      "  Using poisson distribution on request generation\n",
      "  Using synchronous calls for inference\n",
      "\n",
      "Request Rate: 20 inference requests per second\n"
     ]
    }
   ],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# Estimated service rate μ (inferences/sec)\n",
    "MU=100\n",
    "\n",
    "SUMMARY_FILE=\"rho_summary.csv\"\n",
    "\n",
    "# Write header if file doesn't exist\n",
    "if [ ! -f \"$SUMMARY_FILE\" ]; then\n",
    "  echo \"rho,lambda,inferences_per_sec,server_queue,p50_latency\" > \"$SUMMARY_FILE\"\n",
    "fi\n",
    "\n",
    "# Loop over rho values\n",
    "for RHO in 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9; do\n",
    "  LAMBDA=$(awk \"BEGIN {printf \\\"%d\\\", $RHO * $MU}\")\n",
    "  OUTFILE=\"temp_rho_${RHO}.csv\"\n",
    "\n",
    "  echo \"▶ Running for ρ=$RHO (λ=$LAMBDA req/sec) at fixed concurrency $FIXED_CONCURRENCY\"\n",
    "\n",
    "  perf_analyzer -u \"$SERVER_URL\" \\\n",
    "    -m food_classifier_onnx \\\n",
    "    -b 1 \\\n",
    "    --shape IMAGE:3,224,224 \\\n",
    "    --request-rate-range ${LAMBDA}:${LAMBDA}:1 \\\n",
    "    --request-distribution=poisson \\\n",
    "    --collect-metrics \\\n",
    "    -f \"$OUTFILE\"\n",
    "\n",
    "  if [ -f \"$OUTFILE\" ]; then\n",
    "    LINE=$(tail -n +2 \"$OUTFILE\" | head -1)\n",
    "    if [ -n \"$LINE\" ]; then\n",
    "      INF_SEC=$(echo \"$LINE\" | cut -d',' -f2)\n",
    "      SERVER_QUEUE=$(echo \"$LINE\" | cut -d',' -f6)\n",
    "      P50_LATENCY=$(echo \"$LINE\" | cut -d',' -f12)\n",
    "      echo \"$RHO,$LAMBDA,$INF_SEC,$SERVER_QUEUE,$P50_LATENCY\" >> \"$SUMMARY_FILE\"\n",
    "      echo \"Logged for ρ=$RHO\"\n",
    "    else\n",
    "      echo \"No stable data at ρ=$RHO\"\n",
    "    fi\n",
    "  else\n",
    "    echo \"Failed to run perf_analyzer for ρ=$RHO\"\n",
    "  fi\n",
    "done\n",
    "\n",
    "echo \"Done! Summary in $SUMMARY_FILE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average latency and throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Latency limit: 0 msec\n",
      "  Request Rate limit: 99 requests per seconds\n",
      "  Using poisson distribution on request generation\n",
      "  Using synchronous calls for inference\n",
      "\n",
      "Request Rate: 11 inference requests per second\n",
      "  Client: \n",
      "    Request count: 161\n",
      "    Throughput: 8.89008 infer/sec\n",
      "    Avg latency: 10213 usec (standard deviation 1569 usec)\n",
      "    p50 latency: 9825 usec\n",
      "    p90 latency: 10831 usec\n",
      "    p95 latency: 14911 usec\n",
      "    p99 latency: 17069 usec\n",
      "    Avg HTTP time: 10203 usec (send/recv 1061 usec + response wait 9142 usec)\n",
      "  Server: \n",
      "    Inference count: 161\n",
      "    Execution count: 161\n",
      "    Successful request count: 161\n",
      "    Avg request latency: 7207 usec (overhead 43 usec + queue 476 usec + compute input 202 usec + compute infer 6458 usec + compute output 27 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 2.5%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5573 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 34.2044 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 687865856 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request Rate: 22 inference requests per second\n",
      "  Client: \n",
      "    Request count: 390\n",
      "    Throughput: 21.2148 infer/sec\n",
      "    Avg latency: 10640 usec (standard deviation 2172 usec)\n",
      "    p50 latency: 9857 usec\n",
      "    p90 latency: 13315 usec\n",
      "    p95 latency: 15711 usec\n",
      "    p99 latency: 19352 usec\n",
      "    Avg HTTP time: 10631 usec (send/recv 1059 usec + response wait 9572 usec)\n",
      "  Server: \n",
      "    Inference count: 390\n",
      "    Execution count: 390\n",
      "    Successful request count: 390\n",
      "    Avg request latency: 7573 usec (overhead 44 usec + queue 776 usec + compute input 197 usec + compute infer 6530 usec + compute output 26 usec)\n",
      "  Server Prometheus Metrics: \n",
      "    Avg GPU Utilization:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 0%\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 3.44444%\n",
      "    Avg GPU Power Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 32.5562 watts\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 35.6527 watts\n",
      "    Max GPU Memory Usage:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 335544320 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 687865856 bytes\n",
      "    Total GPU Memory:\n",
      "      GPU-81207bda-7e38-0495-0510-11595cdbff2c : 17179869184 bytes\n",
      "      GPU-f48590ef-3a79-f9ee-d4d8-d17034f510d5 : 17179869184 bytes\n",
      "Request Rate: 33 inference requests per second\n",
      "Failed to obtain stable measurement.\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Request Rate: 11, throughput: 8.89008 infer/sec, latency 10213 usec\n",
      "Request Rate: 22, throughput: 21.2148 infer/sec, latency 10640 usec\n"
     ]
    }
   ],
   "source": [
    "perf_analyzer -u triton_server:8000 \\\n",
    "  -m food_classifier_onnx \\\n",
    "  -b 1 \\\n",
    "  --shape IMAGE:3,224,224 \\\n",
    "  --request-rate-range 11:99:11 \\\n",
    "  --request-distribution=poisson \\\n",
    "  --collect-metrics \\\n",
    "  -f test_output.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has much better inference performance than our PyTorch model with Python backend did, in a similar test. Also, if we monitor with `nvtop`, we should see higher GPU utilization while the test is running (which is a good thing!) (Take a screenshot!)\n",
    "\n",
    "Let’s try scaling *this* model up. Edit the model configuration:\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "nano ~/serve-system-chi/models/food_classifier_onnx/config.pbtxt\n",
    "```\n",
    "\n",
    "and change\n",
    "\n",
    "      instance_group [\n",
    "        {\n",
    "          count: 1\n",
    "          kind: KIND_GPU\n",
    "          gpus: [ 0 ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "to\n",
    "\n",
    "      instance_group [\n",
    "        {\n",
    "          count: 2      \n",
    "          kind: KIND_GPU\n",
    "          gpus: [ 0, 1 ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "Save the file (use Ctrl+O then Enter, then Ctrl+X).\n",
    "\n",
    "Re-build the container image with this change:\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "docker compose -f ~/serve-system-chi/docker/docker-compose-triton.yaml build triton_server\n",
    "```\n",
    "\n",
    "and then bring the server back up:\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "docker compose -f ~/serve-system-chi/docker/docker-compose-triton.yaml up triton_server --force-recreate -d\n",
    "```\n",
    "\n",
    "and use\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "docker logs triton_server\n",
    "```\n",
    "\n",
    "to make sure the server comes up and is ready.\n",
    "\n",
    "Then, run our benchmark with higher concurrency. (2 instances on each GPU, because we noticed that a single instance used less than half a GPU.)\n",
    "\n",
    "Watch the `nvtop` output as you run this test! (Take a screenshot!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs inside Jupyter container\n",
    "perf_analyzer -u triton_server:8000  -m food_classifier_onnx -b 1 --shape IMAGE:3,224,224 --concurrency-range 8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \n",
    "\n",
    "    Avg request latency: 3961 usec (overhead 18 usec + queue 697 usec + compute input 97 usec + compute infer 3137 usec + compute output 11 usec)\n",
    "\n",
    "Inferences/Second vs. Client Average Batch Latency\n",
    "Concurrency: 8, throughput: 1182.39 infer/sec, latency 6089 usec\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we should see that our model is fully utilizing the GPU (that’s good!) And, our inference performance is much better than the PyTorch model with Python backend could achieve with concurrency 8.\n",
    "\n",
    "Let’s see how we do with even higher concurrency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs inside Jupyter container\n",
    "perf_analyzer -u triton_server:8000  -m food_classifier_onnx -b 1 --shape IMAGE:3,224,224 --concurrency-range 16  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \n",
    "\n",
    "\n",
    "\n",
    "    Avg request latency: 9960 usec (overhead 19 usec + queue 6793 usec + compute input 100 usec + compute infer 3036 usec + compute output 11 usec)\n",
    "\n",
    "Inferences/Second vs. Client Average Batch Latency\n",
    "Concurrency: 16, throughput: 1257.15 infer/sec, latency 12025 usec\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have some queue delay, since the rate at which requests arrive is greater than the service rate of the models. But, we can feel good that we are no longer underutilizing the GPUs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There’s one more issue we should address: our ONNX model doesn’t directly work with our Flask server now, because the inputs and outputs are different. The ONNX model expects a pre-processed array, and returns a list of class probabilities.\n",
    "\n",
    "Since the pre-processing and post-processing doesn’t need GPU anyway, we’ll move it to the Flask app.\n",
    "\n",
    "Edit the Docker compose file:\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "nano ~/serve-system-chi/docker/docker-compose-triton.yaml\n",
    "```\n",
    "\n",
    "and change\n",
    "\n",
    "      flask:\n",
    "        build:\n",
    "          context: https://github.com/teaching-on-testbeds/gourmetgram.git#triton\n",
    "\n",
    "to\n",
    "\n",
    "      flask:\n",
    "        build:\n",
    "          context: https://github.com/teaching-on-testbeds/gourmetgram.git#triton_onnx\n",
    "\n",
    "to use [a version of our Flask app where the pre- and post-processing is built in](https://github.com/teaching-on-testbeds/gourmetgram/blob/triton_onnx/app.py). Also change\n",
    "\n",
    "          - FOOD11_MODEL_NAME=food_classifier\n",
    "\n",
    "to\n",
    "\n",
    "          - FOOD11_MODEL_NAME=food_classifier_onnx\n",
    "\n",
    "so that our Flask app will send requests to the new ONNX model service.\n",
    "\n",
    "Then run\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "docker compose -f ~/serve-system-chi/docker/docker-compose-triton.yaml build flask\n",
    "```\n",
    "\n",
    "to re-build the container image, and\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "docker compose -f ~/serve-system-chi/docker/docker-compose-triton.yaml up flask --force-recreate -d\n",
    "```\n",
    "\n",
    "to restart the Flask container with the new image.\n",
    "\n",
    "Let’s test this service. In a browser, run\n",
    "\n",
    "    http://A.B.C.D\n",
    "\n",
    "but substitute the floating IP assigned to your instance, to access the Flask app. Upload an image and press “Submit” to get its class label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, download this entire notebook for later reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average latency and throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Using uniform distribution on request generation\n",
      "  Using synchronous calls for inference\n",
      "\n",
      "Request Rate: 100 inference requests per second\n",
      "  Client: \n",
      "    Request count: 1927\n",
      "    Throughput: 100.003 infer/sec\n",
      "    Avg latency: 8476 usec (standard deviation 639 usec)\n",
      "    p50 latency: 8722 usec\n",
      "    p90 latency: 8888 usec\n",
      "    p95 latency: 9036 usec\n",
      "    p99 latency: 9330 usec\n",
      "    Avg HTTP time: 8467 usec (send/recv 380 usec + response wait 8087 usec)\n",
      "  Server: \n",
      "    Inference count: 1927\n",
      "    Execution count: 1927\n",
      "    Successful request count: 1927\n",
      "    Avg request latency: 6353 usec (overhead 43 usec + queue 97 usec + compute input 232 usec + compute infer 5955 usec + compute output 25 usec)\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Request Rate: 100, throughput: 100.003 infer/sec, latency 8476 usec\n"
     ]
    }
   ],
   "source": [
    "perf_analyzer -u triton_server:8000  -m food_classifier_onnx -b 1 --shape IMAGE:3,224,224 \\\n",
    "--request-distribution=constant --request-rate-range 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average latency and throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Using poisson distribution on request generation\n",
      "  Using synchronous calls for inference\n",
      "\n",
      "Request Rate: 100 inference requests per second\n",
      "  Client: \n",
      "    Request count: 1832\n",
      "    Avg send request rate: 99.36 infer/sec\n",
      "    [WARNING] Perf Analyzer was not able to keep up with the desired request rate. 3.77% of the requests were delayed. \n",
      "    Throughput: 97.53 infer/sec\n",
      "    Avg latency: 10952 usec (standard deviation 3851 usec)\n",
      "    p50 latency: 9494 usec\n",
      "    p90 latency: 16554 usec\n",
      "    p95 latency: 18753 usec\n",
      "    p99 latency: 22897 usec\n",
      "    Avg HTTP time: 10943 usec (send/recv 742 usec + response wait 10201 usec)\n",
      "  Server: \n",
      "    Inference count: 1832\n",
      "    Execution count: 1832\n",
      "    Successful request count: 1832\n",
      "    Avg request latency: 8080 usec (overhead 37 usec + queue 2816 usec + compute input 222 usec + compute infer 4983 usec + compute output 21 usec)\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Request Rate: 100.00, throughput: 97.53 infer/sec, latency 10952 usec\n"
     ]
    }
   ],
   "source": [
    "perf_analyzer -u triton_server:8000  -m food_classifier_onnx -b 1 --shape IMAGE:3,224,224 \\\n",
    "    --request-distribution=poisson --request-rate-range 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average latency and throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Using poisson distribution on request generation\n",
      "  Using synchronous calls for inference\n",
      "\n",
      "Request Rate: 200 inference requests per second\n",
      "  Client: \n",
      "    Request count: 3918\n",
      "    Avg send request rate: 203.79 infer/sec\n",
      "    [WARNING] Perf Analyzer was not able to keep up with the desired request rate. 18.27% of the requests were delayed. \n",
      "    Throughput: 196.63 infer/sec\n",
      "    Avg latency: 9522 usec (standard deviation 3035 usec)\n",
      "    p50 latency: 9128 usec\n",
      "    p90 latency: 13364 usec\n",
      "    p95 latency: 14019 usec\n",
      "    p99 latency: 16278 usec\n",
      "    Avg HTTP time: 9513 usec (send/recv 678 usec + response wait 8835 usec)\n",
      "  Server: \n",
      "    Inference count: 3918\n",
      "    Execution count: 3918\n",
      "    Successful request count: 3918\n",
      "    Avg request latency: 6745 usec (overhead 25 usec + queue 3241 usec + compute input 168 usec + compute infer 3296 usec + compute output 14 usec)\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Request Rate: 200.00, throughput: 196.63 infer/sec, latency 9522 usec\n"
     ]
    }
   ],
   "source": [
    "perf_analyzer -u triton_server:8000  -m food_classifier_onnx -b 1 --shape IMAGE:3,224,224 \\\n",
    "    --request-distribution=poisson --request-rate-range 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average latency and throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Using poisson distribution on request generation\n",
      "  Using synchronous calls for inference\n",
      "\n",
      "Request Rate: 500 inference requests per second\n",
      "  Client: \n",
      "    Request count: 8779\n",
      "    Avg send request rate: 437.15 infer/sec\n",
      "    [WARNING] Perf Analyzer was not able to keep up with the desired request rate. 99.92% of the requests were delayed. \n",
      "    Throughput: 372.62 infer/sec\n",
      "    Avg latency: 10006 usec (standard deviation 1103 usec)\n",
      "    p50 latency: 10286 usec\n",
      "    p90 latency: 10522 usec\n",
      "    p95 latency: 10758 usec\n",
      "    p99 latency: 13163 usec\n",
      "    Avg HTTP time: 9998 usec (send/recv 984 usec + response wait 9014 usec)\n",
      "  Server: \n",
      "    Inference count: 8780\n",
      "    Execution count: 8780\n",
      "    Successful request count: 8780\n",
      "    Avg request latency: 7278 usec (overhead 18 usec + queue 4609 usec + compute input 105 usec + compute infer 2535 usec + compute output 10 usec)\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Request Rate: 500.00, throughput: 372.62 infer/sec, latency 10006 usec\n"
     ]
    }
   ],
   "source": [
    "perf_analyzer -u triton_server:8000  -m food_classifier_onnx -b 1 --shape IMAGE:3,224,224 \\\n",
    "    --request-distribution=poisson --request-rate-range 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: perf_analyzer [options]\n",
      "==== SYNOPSIS ====\n",
      " \n",
      "\t--version \n",
      "\t-m <model name>\n",
      "\t-x <model version>\n",
      "\t--bls-composing-models <string>\n",
      "\t--model-signature-name <model signature name>\n",
      "\t--service-kind <\"triton\"|\"openai\"|\"tfserving\"|\"torchserve\"|\"triton_c_api\">\n",
      "\t--endpoint <string>\n",
      "\t-v\n",
      "\n",
      "I. MEASUREMENT PARAMETERS: \n",
      "\t--async (-a)\n",
      "\t--sync\n",
      "\t--measurement-interval (-p) <measurement window (in msec)>\n",
      "\t--concurrency-range <start:end:step>\n",
      "\t--periodic-concurrency-range <start:end:step>\n",
      "\t--session-concurrency <session concurrency>\n",
      "\t--request-period <number of responses>\n",
      "\t--request-rate-range <start:end:step>\n",
      "\t--request-distribution <\"poisson\"|\"constant\">\n",
      "\t--request-intervals <path to file containing time intervals in microseconds>\n",
      "\t--serial-sequences\n",
      "\t--binary-search\n",
      "\t--num-of-sequences <number of concurrent sequences>\n",
      "\t--latency-threshold (-l) <latency threshold (in msec)>\n",
      "\t--max-threads <thread counts>\n",
      "\t--stability-percentage (-s) <deviation threshold for stable measurement (in percentage)>\n",
      "\t--max-trials (-r)  <maximum number of measurements for each profiling>\n",
      "\t--percentile <percentile>\n",
      "\t--request-count <number of requests>\n",
      "\t--warmup-request-count <number of warmup requests>\n",
      "\t--fixed-schedule\n",
      "\tDEPRECATED OPTIONS\n",
      "\t-t <number of concurrent requests>\n",
      "\t-c <maximum concurrency>\n",
      "\t-d\n",
      "\n",
      "II. INPUT DATA OPTIONS: \n",
      "\t-b <batch size>\n",
      "\t--input-data <\"zero\"|\"random\"|<path>>\n",
      "\t--shared-memory <\"system\"|\"cuda\"|\"none\">\n",
      "\t--output-shared-memory-size <size in bytes>\n",
      "\t--shape <name:shape>\n",
      "\t--sequence-length <length>\n",
      "\t--sequence-length-variation <variation>\n",
      "\t--sequence-id-range <start:end>\n",
      "\t--string-length <length>\n",
      "\t--string-data <string>\n",
      "\t--input-tensor-format [binary|json]\n",
      "\t--output-tensor-format [binary|json]\n",
      "\tDEPRECATED OPTIONS\n",
      "\t-z\n",
      "\t--data-directory <path>\n",
      "\n",
      "III. SERVER DETAILS: \n",
      "\t-u <URL for inference service>\n",
      "\t-i <Protocol used to communicate with inference service>\n",
      "\t--ssl-grpc-use-ssl <bool>\n",
      "\t--ssl-grpc-root-certifications-file <path>\n",
      "\t--ssl-grpc-private-key-file <path>\n",
      "\t--ssl-grpc-certificate-chain-file <path>\n",
      "\t--ssl-https-verify-peer <number>\n",
      "\t--ssl-https-verify-host <number>\n",
      "\t--ssl-https-ca-certificates-file <path>\n",
      "\t--ssl-https-client-certificate-file <path>\n",
      "\t--ssl-https-client-certificate-type <string>\n",
      "\t--ssl-https-private-key-file <path>\n",
      "\t--ssl-https-private-key-type <string>\n",
      "\n",
      "IV. OTHER OPTIONS: \n",
      "\t-f <filename for storing report in csv format>\n",
      "\t--profile-export-file <path>\n",
      "\t-H <HTTP header>\n",
      "\t--streaming\n",
      "\t--grpc-compression-algorithm <compression_algorithm>\n",
      "\t--grpc-method\n",
      "\t--trace-level\n",
      "\t--trace-rate\n",
      "\t--trace-count\n",
      "\t--log-frequency\n",
      "\t--collect-metrics\n",
      "\t--metrics-url\n",
      "\t--metrics-interval\n",
      "\n",
      "==== OPTIONS ==== \n",
      " \n",
      " --version: print the current version of Perf Analyzer.\n",
      " -m:     This is a required argument and is used to specify the model against\n",
      "\t which to run perf_analyzer.\n",
      " -x:     The version of the above model to be used. If not specified the most\n",
      "\t recent version (that is, the highest numbered version) of the model\n",
      "\t will be used.\n",
      " --model-signature-name: The signature name of the saved model to use. Default\n",
      "\t value is \"serving_default\". This option will be ignored if\n",
      "\t --service-kind is not \"tfserving\".\n",
      " --service-kind: Describes the kind of service perf_analyzer to generate load\n",
      "\t for. The options are \"triton\", \"openai\", \"triton_c_api\", \"tfserving\"\n",
      "\t and \"torchserve\". Default value is \"triton\". Note in order to use\n",
      "\t \"openai\" you must specify an endpoint via --endpoint. Note in order\n",
      "\t to use \"torchserve\" backend --input-data option must point to a json\n",
      "\t file holding data in the following format {\"data\" :\n",
      "\t [{\"TORCHSERVE_INPUT\" : [\"<complete path to the content file>\"]}, {...}...]}. The\n",
      "\t type of file here will depend on the model. In order to use\n",
      "\t \"triton_c_api\" you must specify the Triton server install path and the model\n",
      "\t repository path via the --triton-server-directory and\n",
      "\t --model-repository flags\n",
      " --endpoint: Describes what endpoint to send requests to on the server. This\n",
      "\t is required when using \"openai\" service-kind, and is ignored for all\n",
      "\t other cases. Currently only \"v1/chat/completions\" is confirmed to\n",
      "\t work.\n",
      " -v:     Enables verbose mode.\n",
      " -v -v:  Enables extra verbose mode.\n",
      "\n",
      "I. MEASUREMENT PARAMETERS: \n",
      " --async (-a): Enables asynchronous mode in perf_analyzer. By default,\n",
      "\t perf_analyzer will use synchronous API to request inference. However, if\n",
      "\t the model is sequential then default mode is asynchronous. Specify\n",
      "\t --sync to operate sequential models in synchronous mode. In synchronous\n",
      "\t mode, perf_analyzer will start threads equal to the concurrency\n",
      "\t level. Use asynchronous mode to limit the number of threads, yet\n",
      "\t maintain the concurrency.\n",
      " --sync: Force enables synchronous mode in perf_analyzer. Can be used to\n",
      "\t operate perf_analyzer with sequential model in synchronous mode.\n",
      " --measurement-interval (-p): Indicates the time interval used for each\n",
      "\t measurement in milliseconds. The perf analyzer will sample a time interval\n",
      "\t specified by -p and take measurement over the requests completed\n",
      "\t within that time interval. The default value is 5000 msec.\n",
      " --measurement-mode <\"time_windows\"|\"count_windows\">: Indicates the mode used\n",
      "\t for stabilizing measurements. \"time_windows\" will create windows\n",
      "\t such that the length of each window is equal to --measurement-interval.\n",
      "\t \"count_windows\" will create windows such that there are at least\n",
      "\t --measurement-request-count requests in each window.\n",
      " --measurement-request-count: Indicates the minimum number of requests to be\n",
      "\t collected in each measurement window when \"count_windows\" mode is\n",
      "\t used. This mode can be enabled using the --measurement-mode flag.\n",
      " --concurrency-range <start:end:step>: Determines the range of concurrency\n",
      "\t levels covered by the perf_analyzer. The perf_analyzer will start from\n",
      "\t the concurrency level of 'start' and go till 'end' with a stride of\n",
      "\t 'step'. The default value of 'end' and 'step' are 1. If 'end' is not\n",
      "\t specified then perf_analyzer will run for a single concurrency\n",
      "\t level determined by 'start'. If 'end' is set as 0, then the concurrency\n",
      "\t limit will be incremented by 'step' till latency threshold is met.\n",
      "\t 'end' and --latency-threshold can not be both 0 simultaneously. 'end'\n",
      "\t can not be 0 for sequence models while using asynchronous mode.\n",
      " --periodic-concurrency-range <start:end:step>: Determines the range of\n",
      "\t concurrency levels in the similar but slightly different manner as the\n",
      "\t --concurrency-range. Perf Analyzer will start from the concurrency\n",
      "\t level of 'start' and increase by 'step' each time. Unlike\n",
      "\t --concurrency-range, the 'end' indicates the *total* number of concurrency since\n",
      "\t the 'start' (including) and will stop increasing once the cumulative\n",
      "\t number of concurrent requests has reached the 'end'. The user can\n",
      "\t specify *when* to periodically increase the concurrency level using\n",
      "\t the --request-period option. The concurrency level will periodically\n",
      "\t increase for every n-th response specified by --request-period. Since\n",
      "\t this disables stability check in Perf Analyzer and reports response\n",
      "\t timestamps only, the user must provide --profile-export-file to\n",
      "\t specify where to dump all the measured timestamps. The default values\n",
      "\t of 'start', 'end', and 'step' are 1.\n",
      " --session-concurrency <session concurrency>: Enables session concurrency\n",
      "\t inference load mode and specifies the number of concurrent multi-turn\n",
      "\t chat sessions to run during the benchmark. A dataset must be provided\n",
      "\t using --input-data with at least as many unique sessions as the\n",
      "\t specified session concurrency. Only supported with\n",
      "\t --service-kind=openai.\n",
      " --request-period <n>: Indicates the number of responses that each request\n",
      "\t must receive before new, concurrent requests are sent when\n",
      "\t --periodic-concurrency-range is specified. Default value is 10.\n",
      " --request-parameter <name:value:type>: Specifies a custom parameter that can\n",
      "\t be sent to a Triton backend as part of the request. For example,\n",
      "\t providing '--request-parameter max_tokens:256:int' to the command line\n",
      "\t will set an additional parameter 'max_tokens' of type 'int' to 256\n",
      "\t as part of the request. The --request-parameter may be specified\n",
      "\t multiple times for different custom parameters.\n",
      " --request-rate-range <start:end:step>: Determines the range of request rates\n",
      "\t for load generated by analyzer. This option can take floating-point\n",
      "\t values. The search along the request rate range is enabled only when\n",
      "\t using this option. If not specified, then analyzer will search\n",
      "\t along the concurrency-range. The perf_analyzer will start from the\n",
      "\t request rate of 'start' and go till 'end' with a stride of 'step'. The\n",
      "\t default values of 'start', 'end' and 'step' are all 1.0. If 'end' is\n",
      "\t not specified then perf_analyzer will run for a single request rate\n",
      "\t as determined by 'start'. If 'end' is set as 0.0, then the request\n",
      "\t rate will be incremented by 'step' till latency threshold is met.\n",
      "\t 'end' and --latency-threshold can not be both 0 simultaneously.\n",
      " --request-distribution [constant|poisson]: Specifies the time interval\n",
      "\t distribution between dispatching inference requests to the server. Poisson\n",
      "\t distribution closely mimics the real-world work load on a server.\n",
      "\t This option is ignored if not using --request-rate-range. By default,\n",
      "\t this option is set to be constant.\n",
      " --request-intervals: Specifies a path to a file containing time intervals in\n",
      "\t microseconds. Each time interval should be in a new line. The\n",
      "\t analyzer will try to maintain time intervals between successive generated\n",
      "\t requests to be as close as possible in this file. This option can be\n",
      "\t used to apply custom load to server with a certain pattern of\n",
      "\t interest. The analyzer will loop around the file if the duration of\n",
      "\t execution exceeds to that accounted for by the intervals. This option can\n",
      "\t not be used with --request-rate-range or --concurrency-range.\n",
      " --binary-search: Enables the binary search on the specified search range.\n",
      "\t This option requires 'start' and 'end' to be expilicitly specified in\n",
      "\t the --concurrency-range or --request-rate-range. When using this\n",
      "\t option, 'step' is more like the precision. Lower the 'step', more the\n",
      "\t number of iterations along the search path to find suitable\n",
      "\t convergence. By default, linear search is used.\n",
      " --num-of-sequences: Sets the number of concurrent sequences for sequence\n",
      "\t models. This option is ignored when --request-rate-range is not\n",
      "\t specified. By default, its value is 4.\n",
      " --latency-threshold (-l): Sets the limit on the observed latency. Analyzer\n",
      "\t will terminate the concurrency search once the measured latency\n",
      "\t exceeds this threshold. By default, latency threshold is set 0 and the\n",
      "\t perf_analyzer will run for entire --concurrency-range.\n",
      " --max-threads: Sets the maximum number of threads that will be created for\n",
      "\t providing desired concurrency or request rate. However, when runningin\n",
      "\t synchronous mode with concurrency-range having explicit 'end'\n",
      "\t specification,this value will be ignored. Default is 4 if\n",
      "\t --request-rate-range is specified otherwise default is 16.\n",
      " --stability-percentage (-s): Indicates the allowed variation in latency\n",
      "\t measurements when determining if a result is stable. The measurement is\n",
      "\t considered as stable if the ratio of max / min from the recent 3\n",
      "\t measurements is within (stability percentage)% in terms of both infer\n",
      "\t per second and latency. Default is 10(%).\n",
      " --max-trials (-r): Indicates the maximum number of measurements for each\n",
      "\t concurrency level visited during search. The perf analyzer will take\n",
      "\t multiple measurements and report the measurement until it is stable.\n",
      "\t The perf analyzer will abort if the measurement is still unstable\n",
      "\t after the maximum number of measurements. The default value is 10.\n",
      " --percentile: Indicates the confidence value as a percentile that will be\n",
      "\t used to determine if a measurement is stable. For example, a value of\n",
      "\t 85 indicates that the 85th percentile latency will be used to\n",
      "\t determine stability. The percentile will also be reported in the results.\n",
      "\t The default is -1 indicating that the average latency is used to\n",
      "\t determine stability\n",
      " --request-count: Specifies a total number of requests to use for measurement.\n",
      "\t The default is 0, which means that there is no request count and\n",
      "\t the measurement will proceed using windows until stabilization is\n",
      "\t detected.\n",
      " --warmup-request-count: Specifies the number of warmup requests to send\n",
      "\t before benchmarking. The default is 0, which means that no warmup\n",
      "\t requests will be sent.\n",
      " --fixed-schedule: Enables fixed schedule inference load mode. In this mode,\n",
      "\t Perf Analyzer runs through the `--input-data` JSON once. Each entry\n",
      "\t represents a request and must have a `timestamp` \"input\", which tells\n",
      "\t Perf Analyzer when to send that request. A timestamp of `N` means\n",
      "\t Perf Analyzer will send that request `N` milliseconds after the start\n",
      "\t of the benchmark.\n",
      " --serial-sequences: Enables serial sequence mode where a maximum of one\n",
      "\t request is outstanding at a time for any given sequence. The default is\n",
      "\t false.\n",
      "\n",
      "II. INPUT DATA OPTIONS: \n",
      " -b:     Batch size for each request sent.\n",
      " --input-data: Select the type of data that will be used for input in\n",
      "\t inference requests. The available options are \"zero\", \"random\", path to a\n",
      "\t directory or a json file. If the option is path to a directory then\n",
      "\t the directory must contain a binary/text file for each\n",
      "\t non-string/string input respectively, named the same as the input. Each file must\n",
      "\t contain the data required for that input for a batch-1 request. Each\n",
      "\t binary file should contain the raw binary representation of the\n",
      "\t input in row-major order for non-string inputs. The text file should\n",
      "\t contain all strings needed by batch-1, each in a new line, listed in\n",
      "\t row-major order. When pointing to a json file, user must adhere to the\n",
      "\t format described in the Performance Analyzer documentation. By\n",
      "\t specifying json data users can control data used with every request.\n",
      "\t Multiple data streams can be specified for a sequence model and the\n",
      "\t analyzer will select a data stream in a round-robin fashion for every\n",
      "\t new sequence. Multiple json files can also be provided (--input-data\n",
      "\t json_file1 --input-data json-file2 and so on) and the analyzer will\n",
      "\t append data streams from each file. When using\n",
      "\t --service-kind=torchserve make sure this option points to a json file. Default is\n",
      "\t \"random\".\n",
      " --shared-memory <\"system\"|\"cuda\"|\"none\">: Specifies the type of the shared\n",
      "\t memory to use for input and output data. Default is none.\n",
      " --output-shared-memory-size: The size in bytes of the shared memory region to\n",
      "\t allocate per output tensor. Only needed when one or more of the\n",
      "\t outputs are of string type and/or variable shape. The value should be\n",
      "\t larger than the size of the largest output tensor the model is\n",
      "\t expected to return. The analyzer will use the following formula to\n",
      "\t calculate the total shared memory to allocate: output_shared_memory_size *\n",
      "\t number_of_outputs * batch_size. Defaults to 100KB.\n",
      " --shape: The shape used for the specified input. The argument must be\n",
      "\t specified as 'name:shape' where the shape is a comma-separated list for\n",
      "\t dimension sizes, for example '--shape input_name:1,2,3' indicate tensor\n",
      "\t shape [ 1, 2, 3 ]. --shape may be specified multiple times to\n",
      "\t specify shapes for different inputs.\n",
      " --sequence-length: Indicates the base length of a sequence used for sequence\n",
      "\t models. A sequence with length X will be composed of X requests to\n",
      "\t be sent as the elements in the sequence. The actual length of the\n",
      "\t sequencewill be within +/- Y% of the base length, where Y defaults to\n",
      "\t 20% and is customizable via `--sequence-length-variation`. If\n",
      "\t sequence length is unspecified and input data is provided, the sequence\n",
      "\t length will be the number of inputs in the user-provided input data.\n",
      "\t Default is 20.\n",
      " --sequence-length-variation: The percentage variation in length of sequences.\n",
      "\t This flag is only valid when not using user-provided input data or\n",
      "\t when `--sequence-length` is specified while using user-provided\n",
      "\t input data. Default is 20.\n",
      " --sequence-id-range <start:end>: Determines the range of sequence id used by\n",
      "\t the perf_analyzer. The perf_analyzer will start from the sequence id\n",
      "\t of 'start' and go till 'end' (excluded). If 'end' is not specified\n",
      "\t then perf_analyzer will use new sequence id without bounds. If 'end'\n",
      "\t is specified and the concurrency setting may result in maintaining\n",
      "\t a number of sequences more than the range of available sequence id,\n",
      "\t perf analyzer will exit with error due to possible sequence id\n",
      "\t collision. The default setting is start from sequence id 1 and without\n",
      "\t bounds\n",
      " --string-length: Specifies the length of the random strings to be generated\n",
      "\t by the analyzer for string input. This option is ignored if\n",
      "\t --input-data points to a directory. Default is 128.\n",
      " --string-data: If provided, analyzer will use this string to initialize\n",
      "\t string input buffers. The perf analyzer will replicate the given string\n",
      "\t to build tensors of required shape. --string-length will not have any\n",
      "\t effect. This option is ignored if --input-data points to a\n",
      "\t directory.\n",
      " --input-tensor-format=[binary|json]: Specifies Triton inference request input\n",
      "\t tensor format. Only valid when HTTP protocol is used. Default is\n",
      "\t 'binary'.\n",
      " --output-tensor-format=[binary|json]: Specifies Triton inference response\n",
      "\t output tensor format. Only valid when HTTP protocol is used. Default is\n",
      "\t 'binary'.\n",
      "\n",
      "III. SERVER DETAILS: \n",
      " -u:                                  Specify URL to the server. When using triton default is \"localhost:8000\" if using HTTP and\n",
      "\t \"localhost:8001\" if using gRPC. When using tfserving default is\n",
      "\t \"localhost:8500\". \n",
      " -i:                                  The communication protocol to use. The available protocols are gRPC and HTTP. Default is HTTP.\n",
      " --ssl-grpc-use-ssl:                  Bool (true|false) for whether to use encrypted channel to the server. Default false.\n",
      " --ssl-grpc-root-certifications-file: Path to file containing the PEM encoding of the server root certificates.\n",
      " --ssl-grpc-private-key-file:         Path to file containing the PEM encoding of the client's private key.\n",
      " --ssl-grpc-certificate-chain-file:   Path to file containing the PEM encoding of the client's certificate chain.\n",
      " --ssl-https-verify-peer:             Number (0|1) to verify the peer's SSL certificate. See\n",
      "\t https://curl.se/libcurl/c/CURLOPT_SSL_VERIFYPEER.html for the meaning of each value. Default is 1.\n",
      " --ssl-https-verify-host:             Number (0|1|2) to verify the certificate's name against host. See\n",
      "\t https://curl.se/libcurl/c/CURLOPT_SSL_VERIFYHOST.html for the meaning of each value. Default is 2.\n",
      " --ssl-https-ca-certificates-file:    Path to Certificate Authority (CA) bundle.\n",
      " --ssl-https-client-certificate-file: Path to the SSL client certificate.\n",
      " --ssl-https-client-certificate-type: Type (PEM|DER) of the client SSL certificate. Default is PEM.\n",
      " --ssl-https-private-key-file:        Path to the private keyfile for TLS and SSL client cert.\n",
      " --ssl-https-private-key-type:        Type (PEM|DER) of the private key file. Default is PEM.\n",
      "\n",
      "IV. OTHER OPTIONS: \n",
      " -f:     The latency report will be stored in the file named by this option.\n",
      "\t By default, the result is not recorded in a file.\n",
      " --profile-export-file: Specifies the path that the profile export will be generated at. By\n",
      "\t default, the profile export will not be generated.\n",
      " -H:     The header will be added to HTTP requests (ignored for GRPC\n",
      "\t requests). The header must be specified as 'Header:Value'. -H may be\n",
      "\t specified multiple times to add multiple headers.\n",
      " --streaming: Enables the use of streaming API. This flag is only valid with\n",
      "\t gRPC protocol. By default, it is set false.\n",
      " --grpc-compression-algorithm: The compression algorithm to be used by gRPC\n",
      "\t when sending request. Only supported when grpc protocol is being used.\n",
      "\t The supported values are none, gzip, and deflate. Default value is\n",
      "\t none.\n",
      " --trace-level: Specify a trace level. OFF to disable tracing, TIMESTAMPS to\n",
      "\t trace timestamps, TENSORS to trace tensors. It may be specified\n",
      "\t multiple times to trace multiple information. Default is OFF.\n",
      " --trace-rate: Set the trace sampling rate. Default is 1000.\n",
      " --trace-count: Set the number of traces to be sampled. If the value is -1,\n",
      "\t the number of traces to be sampled will not be limited. Default is -1.\n",
      " --log-frequency:  Set the trace log frequency. If the value is 0, Triton will\n",
      "\t only log the trace output to the trace file when shutting down.\n",
      "\t Otherwise, Triton will log the trace output to <trace-file>.<idx> when\n",
      "\t it collects the specified number of traces. For example, if the log\n",
      "\t frequency is 100, when Triton collects the 100-th trace, it logs the\n",
      "\t traces to file <trace-file>.0, and when it collects the 200-th\n",
      "\t trace, it logs the 101-th to the 200-th traces to file <trace-file>.1.\n",
      "\t Default is 0.\n",
      " --triton-server-directory: The Triton server install path. Required by and\n",
      "\t only used when C API is used (--service-kind=triton_c_api).\n",
      "\t eg:--triton-server-directory=/opt/tritonserver.\n",
      " --model-repository: The model repository of which the model is loaded.\n",
      "\t Required by and only used when C API is used\n",
      "\t (--service-kind=triton_c_api). eg:--model-repository=/tmp/host/docker-data/model_unit_test.\n",
      " --verbose-csv: The csv files generated by perf analyzer will include\n",
      "\t additional information.\n",
      " --collect-metrics: Enables collection of server-side inference server\n",
      "\t metrics. Outputs metrics in the csv file generated with the -f option. Must\n",
      "\t enable `--verbose-csv` option to use the `--collect-metrics`.\n",
      " --metrics-url: The URL to query for server-side inference server metrics.\n",
      "\t Default is 'localhost:8002/metrics'.\n",
      " --metrics-interval: How often in milliseconds, within each measurement\n",
      "\t window, to query for server-side inference server metrics. Default is\n",
      "\t 1000.\n",
      " --bls-composing-models: A comma separated list of all BLS composing models\n",
      "\t (with optional model version number after a colon for each) that may\n",
      "\t be called by the input BLS model. For example, 'modelA:3,modelB'\n",
      "\t would specify that modelA and modelB are composing models that may be\n",
      "\t called by the input BLS model, and that modelA will use version 3,\n",
      "\t while modelB's version is unspecified\n",
      " --grpc-method: A fully-qualified gRPC method name in\n",
      "\t '<package>.<service>/<method>' format. The option is only supported by dynamic gRPC service\n",
      "\t kind and is used to identify the RPC to use when sending requests to\n",
      "\t the server.\n",
      "Error: Failed to parse --concurrency-range. Invalid value provided: 4\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "99",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "perf_analyzer -u triton_server:8000  -m food_classifier_onnx -b 1 --shape IMAGE:3,224,224 \\\n",
    "    --request-distribution=poisson --request-rate-range 500 --concurrency-range 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Using poisson distribution on request generation\n",
      "  Using asynchronous calls for inference\n",
      "\n",
      "Request Rate: 500 inference requests per second\n",
      "Warning: Request latency is not stabilizing. Please try lowering the request rate.\n",
      "  Client: \n",
      "    Request count: 7629\n",
      "    Avg send request rate: 510.34 infer/sec\n",
      "    [WARNING] Perf Analyzer was not able to keep up with the desired request rate. 1.82% of the requests were delayed. \n",
      "    Throughput: 374.00 infer/sec\n",
      "    Avg latency: 20344624 usec (standard deviation 20347 usec)\n",
      "    p50 latency: 20293961 usec\n",
      "    p90 latency: 22349032 usec\n",
      "    p95 latency: 22599814 usec\n",
      "    p99 latency: 22829916 usec\n",
      "    Avg HTTP time: 20344738 usec (send/recv 13233 usec + response wait 20331505 usec)\n",
      "  Server: \n",
      "    Inference count: 7628\n",
      "    Execution count: 7628\n",
      "    Successful request count: 7628\n",
      "    Avg request latency: 20307088 usec (overhead 19 usec + queue 20304426 usec + compute input 112 usec + compute infer 2520 usec + compute output 10 usec)\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Request Rate: 500.00, throughput: 374.00 infer/sec, latency 20344624 usec\n"
     ]
    }
   ],
   "source": [
    "perf_analyzer -u triton_server:8000  -m food_classifier_onnx -b 1 --shape IMAGE:3,224,224 \\\n",
    "    --request-distribution=poisson --request-rate-range 500 --async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Using poisson distribution on request generation\n",
      "  Using asynchronous calls for inference\n",
      "\n",
      "Request Rate: 330 inference requests per second\n",
      "Warning: Request latency is not stabilizing. Please try lowering the request rate.\n",
      "  Client: \n",
      "    Request count: 7381\n",
      "    Avg send request rate: 335.79 infer/sec\n",
      "    [WARNING] Perf Analyzer was not able to keep up with the desired request rate. 2.30% of the requests were delayed. \n",
      "    Throughput: 331.94 infer/sec\n",
      "    Avg latency: 27890 usec (standard deviation 33067 usec)\n",
      "    p50 latency: 14914 usec\n",
      "    p90 latency: 84995 usec\n",
      "    p95 latency: 115915 usec\n",
      "    p99 latency: 136416 usec\n",
      "    Avg HTTP time: 27616 usec (send/recv 519 usec + response wait 27097 usec)\n",
      "  Server: \n",
      "    Inference count: 7381\n",
      "    Execution count: 7381\n",
      "    Successful request count: 7381\n",
      "    Avg request latency: 24404 usec (overhead 17 usec + queue 21716 usec + compute input 85 usec + compute infer 2574 usec + compute output 11 usec)\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Request Rate: 330.00, throughput: 331.94 infer/sec, latency 27890 usec\n"
     ]
    }
   ],
   "source": [
    "perf_analyzer -u triton_server:8000  -m food_classifier_onnx -b 1 --shape IMAGE:3,224,224 \\\n",
    "    --request-distribution=poisson --request-rate-range 330 --async"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected waiting time is\n",
    "$$\\frac{1}{2\\mu}\\cdot\\frac{\\rho}{1-\\rho}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.00\n",
      "297.00\n",
      "66.00:297.00:33.00\n"
     ]
    }
   ],
   "source": [
    "rho_val=(0.2 0.9)\n",
    "max_service_rate=330\n",
    "step_size=$(awk -v d=\"$max_service_rate\" 'BEGIN { printf \"%.2f\", 0.1 * d }')\n",
    "request_arg=\"\"\n",
    "for rho in \"${rho_val[@]}\"; do\n",
    "    request_rate=$(awk -v v=\"$rho\" -v d=\"$max_service_rate\" 'BEGIN { printf \"%.2f\", v * d }')\n",
    "    echo $request_rate\n",
    "    request_arg+=\"$request_rate:\"\n",
    "done\n",
    "\n",
    "request_arg+=\"$step_size\"\n",
    "echo $request_arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Latency limit: 0 msec\n",
      "  Request Rate limit: 297 requests per seconds\n",
      "  Using poisson distribution on request generation\n",
      "  Using asynchronous calls for inference\n",
      "\n",
      "Request Rate: 66 inference requests per second\n",
      "  Client: \n",
      "    Request count: 1177\n",
      "    Throughput: 63.4895 infer/sec\n",
      "    Avg latency: 11440 usec (standard deviation 3672 usec)\n",
      "    p50 latency: 9765 usec\n",
      "    p90 latency: 16339 usec\n",
      "    p95 latency: 18821 usec\n",
      "    p99 latency: 25492 usec\n",
      "    Avg HTTP time: 11412 usec (send/recv 1107 usec + response wait 10305 usec)\n",
      "  Server: \n",
      "    Inference count: 1177\n",
      "    Execution count: 1177\n",
      "    Successful request count: 1177\n",
      "    Avg request latency: 8211 usec (overhead 42 usec + queue 2161 usec + compute input 188 usec + compute infer 5792 usec + compute output 26 usec)\n",
      "Request Rate: 99 inference requests per second\n",
      "  Client: \n",
      "    Request count: 1825\n",
      "    Throughput: 96.7787 infer/sec\n",
      "    Avg latency: 11429 usec (standard deviation 4781 usec)\n",
      "    p50 latency: 9641 usec\n",
      "    p90 latency: 17657 usec\n",
      "    p95 latency: 21083 usec\n",
      "    p99 latency: 27059 usec\n",
      "    Avg HTTP time: 11370 usec (send/recv 767 usec + response wait 10603 usec)\n",
      "  Server: \n",
      "    Inference count: 1825\n",
      "    Execution count: 1825\n",
      "    Successful request count: 1825\n",
      "    Avg request latency: 8474 usec (overhead 37 usec + queue 3181 usec + compute input 176 usec + compute infer 5057 usec + compute output 22 usec)\n",
      "Request Rate: 132 inference requests per second\n",
      "  Client: \n",
      "    Request count: 2515\n",
      "    Throughput: 128.997 infer/sec\n",
      "    Avg latency: 11062 usec (standard deviation 5141 usec)\n",
      "    p50 latency: 9369 usec\n",
      "    p90 latency: 17363 usec\n",
      "    p95 latency: 20917 usec\n",
      "    p99 latency: 27703 usec\n",
      "    Avg HTTP time: 10975 usec (send/recv 919 usec + response wait 10056 usec)\n",
      "  Server: \n",
      "    Inference count: 2515\n",
      "    Execution count: 2515\n",
      "    Successful request count: 2515\n",
      "    Avg request latency: 7785 usec (overhead 31 usec + queue 3335 usec + compute input 166 usec + compute infer 4233 usec + compute output 19 usec)\n",
      "Request Rate: 165 inference requests per second\n",
      "  Client: \n",
      "    Request count: 3069\n",
      "    Throughput: 160.481 infer/sec\n",
      "    Avg latency: 10476 usec (standard deviation 5711 usec)\n",
      "    p50 latency: 8744 usec\n",
      "    p90 latency: 16549 usec\n",
      "    p95 latency: 19915 usec\n",
      "    p99 latency: 30082 usec\n",
      "    Avg HTTP time: 10379 usec (send/recv 610 usec + response wait 9769 usec)\n",
      "  Server: \n",
      "    Inference count: 3069\n",
      "    Execution count: 3069\n",
      "    Successful request count: 3069\n",
      "    Avg request latency: 7442 usec (overhead 28 usec + queue 3555 usec + compute input 149 usec + compute infer 3693 usec + compute output 17 usec)\n",
      "Request Rate: 198 inference requests per second\n",
      "  Client: \n",
      "    Request count: 3874\n",
      "    Throughput: 194.673 infer/sec\n",
      "    Avg latency: 11146 usec (standard deviation 6177 usec)\n",
      "    p50 latency: 9449 usec\n",
      "    p90 latency: 18817 usec\n",
      "    p95 latency: 22395 usec\n",
      "    p99 latency: 30327 usec\n",
      "    Avg HTTP time: 11030 usec (send/recv 615 usec + response wait 10415 usec)\n",
      "  Server: \n",
      "    Inference count: 3874\n",
      "    Execution count: 3874\n",
      "    Successful request count: 3874\n",
      "    Avg request latency: 7951 usec (overhead 25 usec + queue 4408 usec + compute input 133 usec + compute infer 3369 usec + compute output 15 usec)\n",
      "Request Rate: 231 inference requests per second\n",
      "  Client: \n",
      "    Request count: 4678\n",
      "    Avg send request rate: 221.94 infer/sec\n",
      "    [WARNING] Perf Analyzer was not able to keep up with the desired request rate. 1.18% of the requests were delayed. \n",
      "    Throughput: 229.95 infer/sec\n",
      "    Avg latency: 9842 usec (standard deviation 5907 usec)\n",
      "    p50 latency: 8436 usec\n",
      "    p90 latency: 16333 usec\n",
      "    p95 latency: 18889 usec\n",
      "    p99 latency: 25715 usec\n",
      "    Avg HTTP time: 9689 usec (send/recv 621 usec + response wait 9068 usec)\n",
      "  Server: \n",
      "    Inference count: 4679\n",
      "    Execution count: 4679\n",
      "    Successful request count: 4679\n",
      "    Avg request latency: 6618 usec (overhead 21 usec + queue 3604 usec + compute input 109 usec + compute infer 2871 usec + compute output 13 usec)\n",
      "Request Rate: 264.00 inference requests per second\n",
      "Warning: Request latency is not stabilizing. Please try lowering the request rate.\n",
      "  Client: \n",
      "    Request count: 5878\n",
      "    Avg send request rate: 259.04 infer/sec\n",
      "    [WARNING] Perf Analyzer was not able to keep up with the desired request rate. 1.94% of the requests were delayed. \n",
      "    Throughput: 261.00 infer/sec\n",
      "    Avg latency: 16725 usec (standard deviation 14691 usec)\n",
      "    p50 latency: 12024 usec\n",
      "    p90 latency: 33588 usec\n",
      "    p95 latency: 44298 usec\n",
      "    p99 latency: 73315 usec\n",
      "    Avg HTTP time: 16556 usec (send/recv 550 usec + response wait 16006 usec)\n",
      "  Server: \n",
      "    Inference count: 5879\n",
      "    Execution count: 5879\n",
      "    Successful request count: 5879\n",
      "    Avg request latency: 13304 usec (overhead 19 usec + queue 10195 usec + compute input 104 usec + compute infer 2973 usec + compute output 12 usec)\n",
      "Request Rate: 297.00 inference requests per second\n",
      "Warning: Request latency is not stabilizing. Please try lowering the request rate.\n",
      "  Client: \n",
      "    Request count: 6876\n",
      "    Avg send request rate: 293.74 infer/sec\n",
      "    [WARNING] Perf Analyzer was not able to keep up with the desired request rate. 2.25% of the requests were delayed. \n",
      "    Throughput: 297.50 infer/sec\n",
      "    Avg latency: 19669 usec (standard deviation 19784 usec)\n",
      "    p50 latency: 12873 usec\n",
      "    p90 latency: 43949 usec\n",
      "    p95 latency: 59652 usec\n",
      "    p99 latency: 85432 usec\n",
      "    Avg HTTP time: 19396 usec (send/recv 664 usec + response wait 18732 usec)\n",
      "  Server: \n",
      "    Inference count: 6877\n",
      "    Execution count: 6877\n",
      "    Successful request count: 6877\n",
      "    Avg request latency: 15617 usec (overhead 18 usec + queue 12746 usec + compute input 105 usec + compute infer 2735 usec + compute output 11 usec)\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Request Rate: 66.00, throughput: 63.49 infer/sec, latency 11440 usec\n",
      "Request Rate: 99.00, throughput: 96.78 infer/sec, latency 11429 usec\n",
      "Request Rate: 132.00, throughput: 129.00 infer/sec, latency 11062 usec\n",
      "Request Rate: 165.00, throughput: 160.48 infer/sec, latency 10476 usec\n",
      "Request Rate: 198.00, throughput: 194.67 infer/sec, latency 11146 usec\n",
      "Request Rate: 231.00, throughput: 229.95 infer/sec, latency 9842 usec\n",
      "Request Rate: 264.00, throughput: 261.00 infer/sec, latency 16725 usec\n",
      "Request Rate: 297.00, throughput: 297.50 infer/sec, latency 19669 usec\n"
     ]
    }
   ],
   "source": [
    "perf_analyzer -u triton_server:8000  -m food_classifier_onnx -b 1 --shape IMAGE:3,224,224 \\\n",
    "    --request-distribution=poisson --request-rate-range $request_arg --async -f result_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Latency limit: 0 msec\n",
      "  Request Rate limit: 297 requests per seconds\n",
      "  Using poisson distribution on request generation\n",
      "  Using asynchronous calls for inference\n",
      "\n",
      "Request Rate: 33 inference requests per second\n",
      "Failed to obtain stable measurement.\n",
      "Failed to obtain stable measurement.\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "2",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "perf_analyzer -u triton_server:8000  -m food_classifier_onnx -b 1 --shape IMAGE:3,224,224 \\\n",
    "    --request-distribution=poisson --request-rate-range 33:297:33 --async -f result_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Using poisson distribution on request generation\n",
      "  Using asynchronous calls for inference\n",
      "\n",
      "Request Rate: 66 inference requests per second\n",
      "  Client: \n",
      "    Request count: 1181\n",
      "    Throughput: 63.4872 infer/sec\n",
      "    Avg latency: 11337 usec (standard deviation 3668 usec)\n",
      "    p50 latency: 9671 usec\n",
      "    p90 latency: 16294 usec\n",
      "    p95 latency: 18700 usec\n",
      "    p99 latency: 25029 usec\n",
      "    Avg HTTP time: 11287 usec (send/recv 1117 usec + response wait 10170 usec)\n",
      "  Server: \n",
      "    Inference count: 1181\n",
      "    Execution count: 1181\n",
      "    Successful request count: 1181\n",
      "    Avg request latency: 8078 usec (overhead 42 usec + queue 2090 usec + compute input 187 usec + compute infer 5732 usec + compute output 26 usec)\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Request Rate: 66, throughput: 63.4872 infer/sec, latency 11337 usec\n"
     ]
    }
   ],
   "source": [
    "perf_analyzer -u triton_server:8000  -m food_classifier_onnx -b 1 --shape IMAGE:3,224,224 \\\n",
    "    --request-distribution=poisson --request-rate-range 66 --async -f result_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_analyzer -u triton_server:8000  -m food_classifier_onnx -b 1 --shape IMAGE:3,224,224 \\\n",
    "    --request-distribution=poisson --request-rate-range 100 --async -f result_test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching Experiment\n",
    "\n",
    "Earlier, we noted that our model can achieve higher throughput with low latency by performing inference on batches of input samples, instead of individual samples. However, our client sends requests with individual samples.\n",
    "\n",
    "To improve performance, we can ask the Triton server to batch incoming requests whenever possible, and send them through the server together instead of a sequence. In other words, if the server is ready to handle the next request, and it finds four requests waiting in the queue, it should serve those four as a batch instead of just taking the next request in line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s edit the model configuration:\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "nano ~/summer2025/models/food_classifier_onnx/config.pbtxt\n",
    "```\n",
    "\n",
    "and at the end, add\n",
    "\n",
    "    dynamic_batching {\n",
    "      preferred_batch_size: [4, 6, 8, 10]\n",
    "      max_queue_delay_microseconds: 100\n",
    "    }\n",
    "\n",
    "Save the file (use Ctrl+O then Enter, then Ctrl+X).\n",
    "\n",
    "Re-build the container image with this change:\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "docker compose -f ~/exp-chi/docker/docker/docker-compose.yaml build triton_server\n",
    "```\n",
    "\n",
    "and then bring the server back up:\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "docker compose -f ~/exp-chi/docker/docker/docker-compose.yaml up triton_server --force-recreate -d\n",
    "```\n",
    "\n",
    "and use\n",
    "\n",
    "``` bash\n",
    "# runs on node-serve-system\n",
    "docker logs triton_server\n",
    "```\n",
    "\n",
    "to make sure the server comes up and is ready.\n",
    "\n",
    "Before we benchmark this service again, let’s get some pre-benchmark stats about how many requests have been served, broken down by batch size. (If you’ve just restarted the server, it would be zero!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"model_stats\":[{\"name\":\"food_classifier_onnx\",\"version\":\"1\",\"last_inference\":0,\"inference_count\":0,\"execution_count\":0,\"inference_stats\":{\"success\":{\"count\":0,\"ns\":0},\"fail\":{\"count\":0,\"ns\":0},\"queue\":{\"count\":0,\"ns\":0},\"compute_input\":{\"count\":0,\"ns\":0},\"compute_infer\":{\"count\":0,\"ns\":0},\"compute_output\":{\"count\":0,\"ns\":0},\"cache_hit\":{\"count\":0,\"ns\":0},\"cache_miss\":{\"count\":0,\"ns\":0}},\"response_stats\":{},\"batch_stats\":[],\"memory_usage\":[]}]}\n"
     ]
    }
   ],
   "source": [
    "# runs inside Jupyter container\n",
    "curl http://triton_server:8000/v2/models/food_classifier_onnx/versions/1/stats "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's run a benchmark. I will run benchmarks for a constant request rate of 100 (constant distribution) for various batch sizes {1,8,10,12,16} while the model's configuration has a set max batch number of 16.\n",
    "I will then change the max batch number back to 1 and rerun a couple of benchmarks. It shouldn't allow me to run anything with a batch size greater than one.\n",
    "\n",
    "Note: Reran first benchmarks after changing config again to save results in csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Using uniform distribution on request generation\n",
      "  Using asynchronous calls for inference\n",
      "\n",
      "Request Rate: 100 inference requests per second\n",
      "  Client: \n",
      "    Request count: 1905\n",
      "    Throughput: 100.001 infer/sec\n",
      "    Avg latency: 9454 usec (standard deviation 2274 usec)\n",
      "    p50 latency: 9490 usec\n",
      "    p90 latency: 10029 usec\n",
      "    p95 latency: 10289 usec\n",
      "    p99 latency: 12677 usec\n",
      "    Avg HTTP time: 9341 usec (send/recv 491 usec + response wait 8850 usec)\n",
      "  Server: \n",
      "    Inference count: 1905\n",
      "    Execution count: 1905\n",
      "    Successful request count: 1905\n",
      "    Avg request latency: 6370 usec (overhead 40 usec + queue 137 usec + compute input 156 usec + compute infer 6011 usec + compute output 25 usec)\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Request Rate: 100, throughput: 100.001 infer/sec, latency 9454 usec\n"
     ]
    }
   ],
   "source": [
    "perf_analyzer -u triton_server:8000  -m food_classifier_onnx -b 1 --shape IMAGE:3,224,224 \\\n",
    "    --request-distribution=constant --request-rate-range 100 --async -f batch1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get stats per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"model_stats\":[{\"name\":\"food_classifier_onnx\",\"version\":\"1\",\"last_inference\":1750104654444,\"inference_count\":3878,\"execution_count\":3878,\"inference_stats\":{\"success\":{\"count\":3878,\"ns\":2822039192172},\"fail\":{\"count\":0,\"ns\":0},\"queue\":{\"count\":3878,\"ns\":2794667220082},\"compute_input\":{\"count\":3878,\"ns\":575425899},\"compute_infer\":{\"count\":3878,\"ns\":26580321067},\"compute_output\":{\"count\":3878,\"ns\":84801843},\"cache_hit\":{\"count\":0,\"ns\":0},\"cache_miss\":{\"count\":0,\"ns\":0}},\"response_stats\":{},\"batch_stats\":[{\"batch_size\":1,\"compute_input\":{\"count\":3878,\"ns\":575425899},\"compute_infer\":{\"count\":3878,\"ns\":26580321067},\"compute_output\":{\"count\":3878,\"ns\":84801843}}],\"memory_usage\":[]}]}\n"
     ]
    }
   ],
   "source": [
    "# runs inside Jupyter container\n",
    "curl http://triton_server:8000/v2/models/food_classifier_onnx/versions/1/stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max Batch Number was set to 1 in the configuration. Now it is changed to 10 so let's run the benchmark again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 8\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Using uniform distribution on request generation\n",
      "  Using asynchronous calls for inference\n",
      "\n",
      "Request Rate: 100 inference requests per second\n",
      "  Client: \n",
      "    Request count: 1988\n",
      "    Throughput: 800.134 infer/sec\n",
      "    Avg latency: 13301 usec (standard deviation 4342 usec)\n",
      "    p50 latency: 12626 usec\n",
      "    p90 latency: 13868 usec\n",
      "    p95 latency: 14726 usec\n",
      "    p99 latency: 29348 usec\n",
      "    Avg HTTP time: 13106 usec (send/recv 4005 usec + response wait 9101 usec)\n",
      "  Server: \n",
      "    Inference count: 15904\n",
      "    Execution count: 1988\n",
      "    Successful request count: 1988\n",
      "    Avg request latency: 6820 usec (overhead 22 usec + queue 200 usec + compute input 572 usec + compute infer 6010 usec + compute output 15 usec)\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Request Rate: 100, throughput: 800.134 infer/sec, latency 13301 usec\n"
     ]
    }
   ],
   "source": [
    "perf_analyzer -u triton_server:8000  -m food_classifier_onnx -b 8 --shape IMAGE:3,224,224 \\\n",
    "    --request-distribution=constant --request-rate-range 100 --async -f batch8.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"model_stats\":[{\"name\":\"food_classifier_onnx\",\"version\":\"1\",\"last_inference\":1750104778051,\"inference_count\":55494,\"execution_count\":10330,\"inference_stats\":{\"success\":{\"count\":10330,\"ns\":21019163113344},\"fail\":{\"count\":0,\"ns\":0},\"queue\":{\"count\":10330,\"ns\":20937878192070},\"compute_input\":{\"count\":10330,\"ns\":4276098761},\"compute_infer\":{\"count\":10330,\"ns\":76586698802},\"compute_output\":{\"count\":10330,\"ns\":168676035},\"cache_hit\":{\"count\":0,\"ns\":0},\"cache_miss\":{\"count\":0,\"ns\":0}},\"response_stats\":{},\"batch_stats\":[{\"batch_size\":1,\"compute_input\":{\"count\":3878,\"ns\":575425899},\"compute_infer\":{\"count\":3878,\"ns\":26580321067},\"compute_output\":{\"count\":3878,\"ns\":84801843}},{\"batch_size\":8,\"compute_input\":{\"count\":6452,\"ns\":3700672862},\"compute_infer\":{\"count\":6452,\"ns\":50006377735},\"compute_output\":{\"count\":6452,\"ns\":83874192}}],\"memory_usage\":[]}]}\n"
     ]
    }
   ],
   "source": [
    "# runs inside Jupyter container\n",
    "curl http://triton_server:8000/v2/models/food_classifier_onnx/versions/1/stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 16\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Using uniform distribution on request generation\n",
      "  Using asynchronous calls for inference\n",
      "\n",
      "Request Rate: 100 inference requests per second\n",
      "Warning: Request latency is not stabilizing. Please try lowering the request rate.\n",
      "  Client: \n",
      "    Request count: 1573\n",
      "    Throughput: 1315.81 infer/sec\n",
      "    Avg latency: 11139137 usec (standard deviation 72627 usec)\n",
      "    p50 latency: 6505055 usec\n",
      "    p90 latency: 18334846 usec\n",
      "    p95 latency: 18869078 usec\n",
      "    p99 latency: 19328583 usec\n",
      "    Avg HTTP time: 11148685 usec (send/recv 4189141 usec + response wait 6959544 usec)\n",
      "  Server: \n",
      "    Inference count: 25168\n",
      "    Execution count: 1573\n",
      "    Successful request count: 1573\n",
      "    Avg request latency: 5502057 usec (overhead 32 usec + queue 5489924 usec + compute input 1233 usec + compute infer 10840 usec + compute output 28 usec)\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Request Rate: 100, throughput: 1315.81 infer/sec, latency 11139137 usec\n"
     ]
    }
   ],
   "source": [
    "perf_analyzer -u triton_server:8000  -m food_classifier_onnx -b 16 --shape IMAGE:3,224,224 \\\n",
    "    --request-distribution=constant --request-rate-range 100 --async -f batch16.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"model_stats\":[{\"name\":\"food_classifier_onnx\",\"version\":\"1\",\"last_inference\":1750104938614,\"inference_count\":159062,\"execution_count\":16803,\"inference_stats\":{\"success\":{\"count\":16803,\"ns\":46389981414662},\"fail\":{\"count\":0,\"ns\":0},\"queue\":{\"count\":16803,\"ns\":46227405651444},\"compute_input\":{\"count\":16803,\"ns\":13195332776},\"compute_infer\":{\"count\":16803,\"ns\":148603962452},\"compute_output\":{\"count\":16803,\"ns\":333853326},\"cache_hit\":{\"count\":0,\"ns\":0},\"cache_miss\":{\"count\":0,\"ns\":0}},\"response_stats\":{},\"batch_stats\":[{\"batch_size\":1,\"compute_input\":{\"count\":3878,\"ns\":575425899},\"compute_infer\":{\"count\":3878,\"ns\":26580321067},\"compute_output\":{\"count\":3878,\"ns\":84801843}},{\"batch_size\":8,\"compute_input\":{\"count\":6452,\"ns\":3700672862},\"compute_infer\":{\"count\":6452,\"ns\":50006377735},\"compute_output\":{\"count\":6452,\"ns\":83874192}},{\"batch_size\":16,\"compute_input\":{\"count\":6473,\"ns\":8919234015},\"compute_infer\":{\"count\":6473,\"ns\":72017263650},\"compute_output\":{\"count\":6473,\"ns\":165177291}}],\"memory_usage\":[]}]}\n"
     ]
    }
   ],
   "source": [
    "# runs inside Jupyter container\n",
    "curl http://triton_server:8000/v2/models/food_classifier_onnx/versions/1/stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 12\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Using uniform distribution on request generation\n",
      "  Using asynchronous calls for inference\n",
      "\n",
      "Request Rate: 100 inference requests per second\n",
      "Warning: Request latency is not stabilizing. Please try lowering the request rate.\n",
      "  Client: \n",
      "    Request count: 2001\n",
      "    Throughput: 1266.01 infer/sec\n",
      "    Avg latency: 11482101 usec (standard deviation 78780 usec)\n",
      "    p50 latency: 15692173 usec\n",
      "    p90 latency: 21291909 usec\n",
      "    p95 latency: 21462857 usec\n",
      "    p99 latency: 21558780 usec\n",
      "    Avg HTTP time: 11474427 usec (send/recv 6894382 usec + response wait 4580045 usec)\n",
      "  Server: \n",
      "    Inference count: 24036\n",
      "    Execution count: 2003\n",
      "    Successful request count: 2003\n",
      "    Avg request latency: 977366 usec (overhead 29 usec + queue 967909 usec + compute input 1083 usec + compute infer 8323 usec + compute output 21 usec)\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Request Rate: 100, throughput: 1266.01 infer/sec, latency 11482101 usec\n"
     ]
    }
   ],
   "source": [
    "perf_analyzer -u triton_server:8000  -m food_classifier_onnx -b 12 --shape IMAGE:3,224,224 \\\n",
    "    --request-distribution=constant --request-rate-range 100 --async -f batch12.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 10\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Using uniform distribution on request generation\n",
      "  Using asynchronous calls for inference\n",
      "\n",
      "Request Rate: 100 inference requests per second\n",
      "  Client: \n",
      "    Request count: 1926\n",
      "    Throughput: 1000.21 infer/sec\n",
      "    Avg latency: 16407 usec (standard deviation 2974 usec)\n",
      "    p50 latency: 15821 usec\n",
      "    p90 latency: 16753 usec\n",
      "    p95 latency: 19954 usec\n",
      "    p99 latency: 32344 usec\n",
      "    Avg HTTP time: 16350 usec (send/recv 4309 usec + response wait 12041 usec)\n",
      "  Server: \n",
      "    Inference count: 19250\n",
      "    Execution count: 1925\n",
      "    Successful request count: 1925\n",
      "    Avg request latency: 8688 usec (overhead 22 usec + queue 459 usec + compute input 822 usec + compute infer 7368 usec + compute output 15 usec)\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Request Rate: 100, throughput: 1000.21 infer/sec, latency 16407 usec\n"
     ]
    }
   ],
   "source": [
    "perf_analyzer -u triton_server:8000  -m food_classifier_onnx -b 10 --shape IMAGE:3,224,224 \\\n",
    "    --request-distribution=constant --request-rate-range 100 --async -f batch10.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 14\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Using uniform distribution on request generation\n",
      "  Using asynchronous calls for inference\n",
      "\n",
      "Request Rate: 100 inference requests per second\n",
      "Warning: Request latency is not stabilizing. Please try lowering the request rate.\n",
      "  Client: \n",
      "    Request count: 1778\n",
      "    Throughput: 1313.12 infer/sec\n",
      "    Avg latency: 4093885 usec (standard deviation 80487 usec)\n",
      "    p50 latency: 4960687 usec\n",
      "    p90 latency: 6151674 usec\n",
      "    p95 latency: 6185497 usec\n",
      "    p99 latency: 6221961 usec\n",
      "    Avg HTTP time: 4096567 usec (send/recv 2113403 usec + response wait 1983164 usec)\n",
      "  Server: \n",
      "    Inference count: 24906\n",
      "    Execution count: 1779\n",
      "    Successful request count: 1779\n",
      "    Avg request latency: 1260688 usec (overhead 26 usec + queue 1250052 usec + compute input 1195 usec + compute infer 9394 usec + compute output 20 usec)\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Request Rate: 100, throughput: 1313.12 infer/sec, latency 4093885 usec\n"
     ]
    }
   ],
   "source": [
    "perf_analyzer -u triton_server:8000  -m food_classifier_onnx -b 14 --shape IMAGE:3,224,224 \\\n",
    "    --request-distribution=constant --request-rate-range 100 --async -f batch14.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 14\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Using uniform distribution on request generation\n",
      "  Using asynchronous calls for inference\n",
      "\n",
      "Request Rate: 100 inference requests per second\n",
      "Failed to obtain stable measurement.\n",
      "Failed to obtain stable measurement.\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "2",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "perf_analyzer -u triton_server:8000  -m food_classifier_onnx -b 14 --shape IMAGE:3,224,224 \\\n",
    "    --request-distribution=constant --request-rate-range 100 --async -f batch14.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 4\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Using uniform distribution on request generation\n",
      "  Using asynchronous calls for inference\n",
      "\n",
      "Request Rate: 100 inference requests per second\n",
      "  Client: \n",
      "    Request count: 1958\n",
      "    Throughput: 399.993 infer/sec\n",
      "    Avg latency: 13504 usec (standard deviation 4333 usec)\n",
      "    p50 latency: 13197 usec\n",
      "    p90 latency: 14326 usec\n",
      "    p95 latency: 15646 usec\n",
      "    p99 latency: 25560 usec\n",
      "    Avg HTTP time: 13333 usec (send/recv 2783 usec + response wait 10550 usec)\n",
      "  Server: \n",
      "    Inference count: 7832\n",
      "    Execution count: 1958\n",
      "    Successful request count: 1958\n",
      "    Avg request latency: 6607 usec (overhead 38 usec + queue 241 usec + compute input 495 usec + compute infer 5808 usec + compute output 24 usec)\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Request Rate: 100, throughput: 399.993 infer/sec, latency 13504 usec\n"
     ]
    }
   ],
   "source": [
    "perf_analyzer -u triton_server:8000  -m food_classifier_onnx -b 4 --shape IMAGE:3,224,224 \\\n",
    "    --request-distribution=constant --request-rate-range 100 --async -f batch4.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 6\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Using uniform distribution on request generation\n",
      "  Using asynchronous calls for inference\n",
      "\n",
      "Request Rate: 100 inference requests per second\n",
      "  Client: \n",
      "    Request count: 1911\n",
      "    Throughput: 600.047 infer/sec\n",
      "    Avg latency: 13628 usec (standard deviation 2321 usec)\n",
      "    p50 latency: 13657 usec\n",
      "    p90 latency: 14445 usec\n",
      "    p95 latency: 14651 usec\n",
      "    p99 latency: 19401 usec\n",
      "    Avg HTTP time: 13517 usec (send/recv 2699 usec + response wait 10818 usec)\n",
      "  Server: \n",
      "    Inference count: 11466\n",
      "    Execution count: 1911\n",
      "    Successful request count: 1911\n",
      "    Avg request latency: 6483 usec (overhead 39 usec + queue 123 usec + compute input 695 usec + compute infer 5600 usec + compute output 26 usec)\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Request Rate: 100, throughput: 600.047 infer/sec, latency 13628 usec\n"
     ]
    }
   ],
   "source": [
    "perf_analyzer -u triton_server:8000  -m food_classifier_onnx -b 6 --shape IMAGE:3,224,224 \\\n",
    "    --request-distribution=constant --request-rate-range 100 --async -f batch6.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changed max batch size to 1 again in config file to see what would happen without batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Using uniform distribution on request generation\n",
      "  Using asynchronous calls for inference\n",
      "\n",
      "Request Rate: 100 inference requests per second\n",
      "  Client: \n",
      "    Request count: 1908\n",
      "    Throughput: 99.9963 infer/sec\n",
      "    Avg latency: 9849 usec (standard deviation 2974 usec)\n",
      "    p50 latency: 9818 usec\n",
      "    p90 latency: 10559 usec\n",
      "    p95 latency: 11348 usec\n",
      "    p99 latency: 12183 usec\n",
      "    Avg HTTP time: 9688 usec (send/recv 597 usec + response wait 9091 usec)\n",
      "  Server: \n",
      "    Inference count: 1908\n",
      "    Execution count: 1908\n",
      "    Successful request count: 1908\n",
      "    Avg request latency: 6479 usec (overhead 42 usec + queue 143 usec + compute input 194 usec + compute infer 6073 usec + compute output 26 usec)\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Request Rate: 100, throughput: 99.9963 infer/sec, latency 9849 usec\n"
     ]
    }
   ],
   "source": [
    "perf_analyzer -u triton_server:8000  -m food_classifier_onnx -b 1 --shape IMAGE:3,224,224 \\\n",
    "    --request-distribution=constant --request-rate-range 100 --async "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 8\n",
      "  Service Kind: TRITON\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Stabilizing using average throughput\n",
      "  Measurement window: 5000 msec\n",
      "  Using uniform distribution on request generation\n",
      "  Using asynchronous calls for inference\n",
      "\n",
      "Request Rate: 100 inference requests per second\n",
      "Failed to retrieve results from inference request.\n",
      "Thread [0] had error: [request id: 0] inference request batch-size must be <= 1 for 'food_classifier_onnx'\n",
      "\n",
      "Thread [1] had error: [request id: 281474976710656] inference request batch-size must be <= 1 for 'food_classifier_onnx'\n",
      "\n",
      "Thread [2] had error: [request id: 562949953421312] inference request batch-size must be <= 1 for 'food_classifier_onnx'\n",
      "\n",
      "Thread [3] had error: [request id: 844424930131968] inference request batch-size must be <= 1 for 'food_classifier_onnx'\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "99",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "perf_analyzer -u triton_server:8000  -m food_classifier_onnx -b 8 --shape IMAGE:3,224,224 \\\n",
    "    --request-distribution=constant --request-rate-range 100 --async "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Tells us it is not possible to do batches of size 8 because the configuration only allows max batch size of 1. Not worth testing bigger batches - since it is not going to work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
